{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaheriar/CS-228-Deep-Learning-Project/blob/Experimental-Changes/CS228FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 228 Final Project\n",
        "## Enhancing Image Captioning with Deep Learning Models\n",
        "### Saul Gonzalez - sgonz081\n",
        "### Shaheriar Malik - smali032\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "Image captioning is a difficult task that is one step above image classification since we are\n",
        "generating an actual text description of each image. So, deep learning would be an obvious choice in this case since generating text for a variable input image is a difficult task that would require a complex model.\n"
      ],
      "metadata": {
        "id": "jNU4I8Ai0yzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fgiadna8xoGq",
        "outputId": "8dd4a29b-4b84-424f-fc4f-11e01e32f2d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n",
        "import os\n",
        "import natsort\n",
        "import cv2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n \"/content/drive/MyDrive/ImageCaptioningDataset/flickr30k_images.zip\" -d \"/content\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "anVSuIG2PNc5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('/content/drive/MyDrive/ImageCaptioningDataset/results.csv', sep='|', header = None)\n",
        "#raw_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "#raw_df.columns = raw_df.columns.str.replace(' ', '')\n",
        "#raw_df.loc[19999,'comment'] = 'A dog runs across the grass .'\n",
        "#raw_df.to_csv('out.csv',sep='|',index=False)\n",
        "#raw_df.iloc[[19999]]"
      ],
      "metadata": {
        "id": "rGJLib9AXtS3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read, resize and zero pad images. Returns image tensor [3, 256, 256]\n",
        "def readImage(path):\n",
        "    desired_size = 256 # 256 x 256\n",
        "    image = cv2.imread(path)\n",
        "    old_size = image.shape[:2]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(new_im)\n",
        "    return tensor\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    num = temp_df.shape[0]\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(num):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n",
        "        image = readImage(f\"/content/flickr30k_images/{temp_df.image_name[i]}\")\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.comment[i], 30)))\n",
        "\n",
        "#num = 5\n",
        "#display_images(raw_df.sample(num))"
      ],
      "metadata": {
        "id": "MfB4KsAgZTrB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary for Captions\n",
        "def build_vocab(captions,tokenizer,max_len):\n",
        "  counter = Counter()\n",
        "  current_max = max_len\n",
        "  for caption in captions:\n",
        "    sentence = caption.strip()\n",
        "    tokens = tokenizer(sentence)\n",
        "    if len(tokens) > current_max:\n",
        "      current_max = len(tokens)\n",
        "    counter.update(tokens)\n",
        "  sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  dictionary = OrderedDict(sorted_by_freq)\n",
        "  dictionary = dict(dictionary)\n",
        "  dictionary = OrderedDict(dictionary)\n",
        "  return vocab(dictionary,specials=['<PAD>','<SOS>','<EOS>','<UNK>']), current_max"
      ],
      "metadata": {
        "id": "E2vsl3IoFom4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_csv, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_len = -1\n",
        "\n",
        "        self.data_df = pd.read_csv(label_csv, sep='|',header=None)\n",
        "        self.data_df = self.data_df.dropna()\n",
        "        #self.data_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "        #self.data_df.columns = self.data_df.columns.str.replace(' ', '')\n",
        "        self.captions = self.data_df[1] # 0 is image_name, 1 is comment\n",
        "        self.captions = self.captions.apply(lambda x: x.lower())\n",
        "        self.captions = self.captions.apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
        "        self.captions = self.captions.apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
        "        self.captions = self.captions.apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.data_df[0]#os.listdir(data_dir)  # Assumes images are directly under data_dir\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.vocab, self.target_len = build_vocab(self.captions.tolist(),self.tokenizer,self.target_len)\n",
        "        self.target_len += 2\n",
        "        self.vocab.set_default_index(self.vocab['<UNK>'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #image = (image - np.mean(image))/np.std(image) # normalize\n",
        "        image = Image.fromarray(image)# * 255).astype(np.uint8))\n",
        "        # = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[idx]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Vectorize the given caption using our dataset's vocabulary\n",
        "        caption = caption.strip()\n",
        "        caption = self.tokenizer(caption)\n",
        "        caption_vector = [self.vocab['<SOS>']]\n",
        "        caption_vector.extend([self.vocab[word] for word in caption])\n",
        "        caption_vector.append(self.vocab['<EOS>'])\n",
        "\n",
        "        # Add padding to the vector if it needs it\n",
        "        if len(caption_vector) < self.target_len:\n",
        "          for i in range(self.target_len - len(caption_vector)):\n",
        "            caption_vector.append(self.vocab['<PAD>'])\n",
        "\n",
        "        # Return the processed image and any associated labels\n",
        "        return image, torch.tensor(caption_vector)"
      ],
      "metadata": {
        "id": "pKYERZ5AUtFM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0,0,0], std=[1,1,1])\n",
        "])\n",
        "\n",
        "data_dir = '/content/flickr30k_images/'\n",
        "label_csv = '/content/drive/MyDrive/ImageCaptioningDataset/results.csv'\n",
        "dataset = CustomDataset(data_dir, label_csv, transform=transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vxtZgwO6e21r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f10b6f-e1ed-4f75-f10c-f8b9a71bcd73"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test,train = random_split(dataset, [int(len(dataset)*0.3)+1, int(len(dataset)*0.7)])\n",
        "print(len(test),len(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFtN18AqHI2",
        "outputId": "cff2046f-6118-437d-a9d6-da5176c9137d"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47675 111240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = raw_df[0]\n",
        "image_paths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZGiNPk2eyir",
        "outputId": "89788993-4b5f-431a-f44c-04820fe44a71"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000092795.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 37\n",
        "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EfbFZUHSVJ16"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "bkHmM37T9822",
        "outputId": "774b335e-39b6-4f9d-b56e-23910359282e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20205"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# i = 0\n",
        "# for img, file_name in dataloader:\n",
        "#   clear_output()\n",
        "#   #print(label[0])\n",
        "#   #print(raw_df['image_name'])\n",
        "#   plt.imshow(img[0].permute(1,2,0))\n",
        "#   print(list(raw_df.loc[raw_df['image_name'] == file_name[0]].comment)[0])\n",
        "#   i+=5\n",
        "#   plt.show()\n",
        "#   time.sleep(1)"
      ],
      "metadata": {
        "id": "8Om2xhbrV8Kf"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder & Decoder"
      ],
      "metadata": {
        "id": "neF3kL8GuT_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,embed_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    # Load the pretrained ResNet-101 model\n",
        "    self.resnet = models.wide_resnet50_2(pretrained=True)\n",
        "    self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
        "\n",
        "  def forward(self,images):\n",
        "    features = self.resnet(images)\n",
        "    features = features.unsqueeze(1)\n",
        "    return features"
      ],
      "metadata": {
        "id": "3haTP7JWud4R"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=dataset.vocab['<PAD>'])\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "    self.attention = nn.Linear(embed_size,embed_size)\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings = self.embedding(captions)\n",
        "    combined = torch.cat((features, embeddings), dim=1)\n",
        "\n",
        "    attention_weights = torch.softmax(self.attention(combined), dim=1)\n",
        "    attention_encoding = torch.sum(combined * attention_weights, dim=1)\n",
        "\n",
        "    lstm_out, _ = self.lstm(combined)\n",
        "    lstm_out = lstm_out[:,1:,:]\n",
        "\n",
        "    outputs = self.fc(lstm_out)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "ymexnplm23lK"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "encoder = Encoder(embed_size).to(device)\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "decoder = Decoder(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUU6kEjQ-9or",
        "outputId": "bb0ac820-1035-45c1-d2b2-1f6c48263c9e"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def training():\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<PAD>'])\n",
        "  optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "  num_epochs = 10\n",
        "  for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    i = 0\n",
        "    for images, captions in train_loader:\n",
        "        images = images.to(device, dtype=torch.float)\n",
        "        captions = captions.to(device)\n",
        "        #print('captions shape',captions.shape)\n",
        "\n",
        "        # Forward pass\n",
        "        features = encoder(images)\n",
        "\n",
        "        outputs = decoder(features, captions)\n",
        "        #outputs = outputs[:,1:,:]\n",
        "        #print('outputs shape',outputs.shape)\n",
        "       # print(outputs)\n",
        "        #print(outputs.reshape(-1, vocab_size).shape)\n",
        "        # Reshape captions for loss calculation\n",
        "        targets = captions[:, :].reshape(-1)\n",
        "        \n",
        "        #print(targets.shape)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss\n",
        "        if i % 100 == 0:\n",
        "          print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "        i+=1\n",
        "\n",
        "training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i_RgnkOBbYw",
        "outputId": "59262517-92a1-4882-8f74-3a8d9973784d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/3007], Loss: 9.9147\n",
            "Epoch [1/10], Step [100/3007], Loss: 2.0386\n",
            "Epoch [1/10], Step [200/3007], Loss: 0.9889\n",
            "Epoch [1/10], Step [300/3007], Loss: 0.9346\n",
            "Epoch [1/10], Step [400/3007], Loss: 0.6406\n",
            "Epoch [1/10], Step [500/3007], Loss: 0.5265\n",
            "Epoch [1/10], Step [600/3007], Loss: 0.4518\n",
            "Epoch [1/10], Step [700/3007], Loss: 0.2610\n",
            "Epoch [1/10], Step [800/3007], Loss: 0.1620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(encoder.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/encoder.pt')"
      ],
      "metadata": {
        "id": "DN4xkuG0SB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(decoder.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/decoder.pt')"
      ],
      "metadata": {
        "id": "0ysTktxKSRo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "encoder = Encoder(embed_size)\n",
        "decoder = Decoder(embed_size, hidden_size, vocab_size)\n",
        "encoder.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/encoder.pt'))\n",
        "decoder.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/decoder.pt'))\n",
        "encoder.to(device)\n",
        "decoder.to(device)"
      ],
      "metadata": {
        "id": "-L0gUb6X1Tra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference_function(image):\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "\n",
        "  features = encoder(image)\n",
        "\n",
        "  caption = [dataset.vocab['<SOS>']]\n",
        "  with torch.no_grad():\n",
        "    while len(caption) < dataset.target_len and caption[-1] != dataset.vocab['<EOS>']:\n",
        "      # features = features.to(device, dtype=torch.float)\n",
        "      captions = torch.tensor(caption).unsqueeze(0).to(device)\n",
        "      #print(captions[0])\n",
        "      outputs = decoder(features, captions)\n",
        "      #print(outputs.argmax(2))\n",
        "      _, pred_words = torch.max(outputs,2)\n",
        "      pred_words = pred_words.cpu().numpy()\n",
        "      caption.append(pred_words[-1][-1])\n",
        "  i = 0\n",
        "  full_caption = []\n",
        "  for idx in caption:\n",
        "    if dataset.vocab.lookup_token(idx) not in ['<PAD','<SOS>','<EOS>']:\n",
        "      #print(dataset.vocab.lookup_token(idx))\n",
        "      full_caption.append(dataset.vocab.lookup_token(idx))\n",
        "      i += 1\n",
        "  #full_caption = [dataset.vocab.lookup_token(idx) for idx in caption if dataset.vocab.lookup_token(idx) not in ['<PAD','<SOS>','<EOS>']]\n",
        "  return full_caption"
      ],
      "metadata": {
        "id": "lftWJ91r1m6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 300\n",
        "normal_image = dataset[k][0]\n",
        "image = dataset[k][0].unsqueeze(0)\n",
        "image = image.to(device, dtype=torch.float)\n",
        "full_caption = inference_function(image)"
      ],
      "metadata": {
        "id": "4YXANGML47OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_caption"
      ],
      "metadata": {
        "id": "9rxUvxdL5t5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(normal_image.permute(1, 2, 0))\n",
        "' '.join(full_caption)"
      ],
      "metadata": {
        "id": "vJTmqRK7_T-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        \n",
        "        # Load the pretrained ResNet-101 model\n",
        "        self.resnet = models.wide_resnet50_2(weights=None)\n",
        "        self.resnet.fc = nn.Linear(2048, embed_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=dataset.vocab['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.attention = nn.Linear(embed_size,embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        features = self.resnet(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        combined = torch.cat((features, embeddings), dim=1)\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(combined), dim=1)\n",
        "        attention_encoding = torch.sum(combined * attention_weights, dim=1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "\n",
        "        outputs = self.fc(lstm_out)\n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6vCRizEKHyg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256  # Size of the word embedding\n",
        "hidden_size = 512  # Size of the LSTM hidden state\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "v04TdT8KIShA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(x, y, a):\n",
        "    lam = np.random.beta(a,a)\n",
        "    rand = torch.randperm(batch_size)\n",
        "    x2 = x[rand,:]\n",
        "    y2 = y[rand,:]\n",
        "    X = lam * x + (1 - lam) * x2\n",
        "    Y = lam * y + (1 - lam) * y2\n",
        "    #print(Y)\n",
        "    return X, Y\n",
        "\n",
        "def cutout(image, k):\n",
        "    x = np.random.randint(0, 128-k)\n",
        "    y = np.random.randint(0, 128-k)\n",
        "    image[:, x:x+k, y:y+k] = 0\n",
        "    return image\n",
        "\n",
        "def standard_augmentation(image,k):\n",
        "    k1 = np.random.randint(-k, k)\n",
        "    k2 = np.random.randint(-k, k)\n",
        "    image = np.roll(image, k1, axis=1)\n",
        "    image = np.roll(image, k2, axis=2)\n",
        "    if (k1 > 0):    \n",
        "        image[:, :k1, :] = 0\n",
        "    else:\n",
        "        image[:, k1:, :] = 0\n",
        "    if (k2 > 0):\n",
        "        image[:, :, :k2] = 0\n",
        "    else:\n",
        "        image[:, :, k2:] = 0\n",
        "    if (np.random.rand() < 0.5):\n",
        "        image = np.flip(image, axis=2)\n",
        "    return image"
      ],
      "metadata": {
        "id": "4Y9TNrlpu_NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<PAD>'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "def training():\n",
        "  for epoch in range(num_epochs):\n",
        "      i = 0\n",
        "      copy_train_loader = deepcopy(train_loader)\n",
        "      for images, captions in copy_train_loader:\n",
        "          for j in range(len(images)):\n",
        "            if (np.random.rand() < 0.5):\n",
        "              images[j] = cutout(images[j], 64)\n",
        "            images[j] = torch.from_numpy(standard_augmentation(images[j],16).copy())\n",
        "          #images, captions = mixup(images, captions, 0.4)\n",
        "          images = images.to(device, dtype=torch.float)\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          # captions[:, :-1]\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # Reshape captions for loss calculation\n",
        "          targets = captions[:, :].reshape(-1)\n",
        "          \n",
        "          # Compute loss\n",
        "          loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print the loss\n",
        "          if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "          i+=1\n",
        "\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)\n",
        "training()"
      ],
      "metadata": {
        "id": "v3eCeIOEIjTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)"
      ],
      "metadata": {
        "id": "umYg9GvvPeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_image = dataset[0][0]\n",
        "image = dataset[0][0].unsqueeze(0)\n",
        "caption = dataset[0][1].unsqueeze(0)\n",
        "image = image.to(device, dtype=torch.float)\n",
        "caption = caption.to(device)"
      ],
      "metadata": {
        "id": "W_lfCl54RXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs = model(image,caption)\n",
        "  _,preds = torch.max(outputs,2)"
      ],
      "metadata": {
        "id": "fJcMPk-URYHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_preds = []\n",
        "for _,num in enumerate(preds[0]):\n",
        "  idx = num.item()\n",
        "  if idx not in dataset.vocab.lookup_indices(['<PAD>','<SOS>','<EOS>']):\n",
        "    translated_preds.append(dataset.vocab.lookup_token(idx))\n",
        "\n",
        "pred_caption = ' '.join(translated_preds)\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "unt9VMcIS9mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_caption = []\n",
        "for _,num in enumerate(caption[0]):\n",
        "  idx = num.item()\n",
        "  if idx not in dataset.vocab.lookup_indices(['<PAD>','<SOS>','<EOS>']):\n",
        "    translated_caption.append(dataset.vocab.lookup_token(idx))"
      ],
      "metadata": {
        "id": "nyuuJu-2TMIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score(translated_preds[:17],translated_caption)"
      ],
      "metadata": {
        "id": "uigTTKFLTB1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(normal_image.permute(1, 2, 0))\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "H3gNp2TbXp7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/model_UPDATED_2.pt')"
      ],
      "metadata": {
        "id": "kjt2WuekBHj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}