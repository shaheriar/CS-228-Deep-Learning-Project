{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaheriar/CS-228-Deep-Learning-Project/blob/Experimental-Changes/CS228FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 228 Final Project\n",
        "## Enhancing Image Captioning with Deep Learning Models\n",
        "### Saul Gonzalez - sgonz081\n",
        "### Shaheriar Malik - smali032\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "Image captioning is a difficult task that is one step above image classification since we are\n",
        "generating an actual text description of each image. So, deep learning would be an obvious choice in this case since generating text for a variable input image is a difficult task that would require a complex model.\n"
      ],
      "metadata": {
        "id": "jNU4I8Ai0yzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "fgiadna8xoGq",
        "outputId": "2f5adaf8-db07-469e-b69c-5895fb1bf223",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n",
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "from copy import deepcopy\n",
        "\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n",
        "import os\n",
        "import natsort\n",
        "import cv2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n \"/content/drive/MyDrive/ImageCaptioningDataset/flickr30k_images.zip\" -d \"/content\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "anVSuIG2PNc5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('/content/drive/MyDrive/ImageCaptioningDataset/results.csv', sep='|', header = None)\n",
        "#raw_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "#raw_df.columns = raw_df.columns.str.replace(' ', '')\n",
        "#raw_df.loc[19999,'comment'] = 'A dog runs across the grass .'\n",
        "#raw_df.to_csv('out.csv',sep='|',index=False)\n",
        "#raw_df.iloc[[19999]]"
      ],
      "metadata": {
        "id": "rGJLib9AXtS3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read, resize and zero pad images. Returns image tensor [3, 256, 256]\n",
        "def readImage(path):\n",
        "    desired_size = 256 # 256 x 256\n",
        "    image = cv2.imread(path)\n",
        "    old_size = image.shape[:2]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(new_im)\n",
        "    return tensor\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    num = temp_df.shape[0]\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(num):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n",
        "        image = readImage(f\"/content/flickr30k_images/{temp_df.image_name[i]}\")\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.comment[i], 30)))\n",
        "\n",
        "#num = 5\n",
        "#display_images(raw_df.sample(num))"
      ],
      "metadata": {
        "id": "MfB4KsAgZTrB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary for Captions\n",
        "def build_vocab(captions,tokenizer,max_len):\n",
        "  counter = Counter()\n",
        "  current_max = max_len\n",
        "  for caption in captions:\n",
        "    sentence = caption.strip()\n",
        "    tokens = tokenizer(sentence)\n",
        "    if len(tokens) > current_max:\n",
        "      current_max = len(tokens)\n",
        "    counter.update(tokens)\n",
        "  sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  dictionary = OrderedDict(sorted_by_freq)\n",
        "  dictionary = dict(dictionary)\n",
        "  dictionary = OrderedDict(dictionary)\n",
        "  return vocab(dictionary,specials=['<PAD>','<SOS>','<EOS>','<UNK>']), current_max"
      ],
      "metadata": {
        "id": "E2vsl3IoFom4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_csv, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_len = -1\n",
        "\n",
        "        self.data_df = pd.read_csv(label_csv, sep='|',header=None)\n",
        "        self.data_df = self.data_df.dropna()\n",
        "        #self.data_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "        #self.data_df.columns = self.data_df.columns.str.replace(' ', '')\n",
        "        self.captions = self.data_df[1] # 0 is image_name, 1 is comment\n",
        "        self.captions = self.captions.apply(lambda x: x.lower())\n",
        "        self.captions = self.captions.apply(lambda x: x.replace(\"[^A-Za-z]\",\"\"))\n",
        "        self.captions = self.captions.apply(lambda x: x.replace(\"\\s+\",\" \"))\n",
        "        self.captions = self.captions.apply(lambda x: \" \".join([word for word in x.split() if len(word)>1]))\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.data_df[0]#os.listdir(data_dir)  # Assumes images are directly under data_dir\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.vocab, self.target_len = build_vocab(self.captions.tolist(),self.tokenizer,self.target_len)\n",
        "        self.target_len += 2\n",
        "        self.vocab.set_default_index(self.vocab['<UNK>'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #image = (image - np.mean(image))/np.std(image) # normalize\n",
        "        image = Image.fromarray(image)# * 255).astype(np.uint8))\n",
        "        # = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[idx]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Vectorize the given caption using our dataset's vocabulary\n",
        "        caption = caption.strip()\n",
        "        caption = self.tokenizer(caption)\n",
        "        caption_vector = [self.vocab['<SOS>']]\n",
        "        caption_vector.extend([self.vocab[word] for word in caption])\n",
        "        caption_vector.append(self.vocab['<EOS>'])\n",
        "\n",
        "        # Add padding to the vector if it needs it\n",
        "        if len(caption_vector) < self.target_len:\n",
        "          for i in range(self.target_len - len(caption_vector)):\n",
        "            caption_vector.append(self.vocab['<PAD>'])\n",
        "\n",
        "        # Return the processed image and any associated labels\n",
        "        return image, torch.tensor(caption_vector)"
      ],
      "metadata": {
        "id": "pKYERZ5AUtFM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4422, 0.4074, 0.3500], std=[0.2814, 0.2803, 0.2798])\n",
        "])\n",
        "\n",
        "data_dir = '/content/flickr30k_images/'\n",
        "label_csv = '/content/drive/MyDrive/ImageCaptioningDataset/results.csv'\n",
        "dataset = CustomDataset(data_dir, label_csv, transform=transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vxtZgwO6e21r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e238bc0e-f8c5-4100-dd8d-c6b4d4aa1d4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test,train = random_split(dataset, [int(len(dataset)*0.3)+1, int(len(dataset)*0.7)])\n",
        "print(len(test),len(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFtN18AqHI2",
        "outputId": "4ef33b01-e127-4ad9-ed8c-f10ec31454fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47675 111240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = raw_df[0]\n",
        "image_paths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZGiNPk2eyir",
        "outputId": "a9fff599-84d9-4ae6-b129-5f4c7744672e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000092795.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 37\n",
        "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EfbFZUHSVJ16"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "bkHmM37T9822",
        "outputId": "e61f92ee-026a-4ab5-9259-a6e3de84902c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20205"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# i = 0\n",
        "# for img, file_name in dataloader:\n",
        "#   clear_output()\n",
        "#   #print(label[0])\n",
        "#   #print(raw_df['image_name'])\n",
        "#   plt.imshow(img[0].permute(1,2,0))\n",
        "#   print(list(raw_df.loc[raw_df['image_name'] == file_name[0]].comment)[0])\n",
        "#   i+=5\n",
        "#   plt.show()\n",
        "#   time.sleep(1)"
      ],
      "metadata": {
        "id": "8Om2xhbrV8Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder & Decoder"
      ],
      "metadata": {
        "id": "neF3kL8GuT_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self,embed_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    # Load the pretrained ResNet-101 model\n",
        "    self.resnet = models.wide_resnet50_2(pretrained=True)\n",
        "    self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
        "\n",
        "  def forward(self,images):\n",
        "    features = self.resnet(images)\n",
        "    features = features.unsqueeze(1)\n",
        "    return features"
      ],
      "metadata": {
        "id": "3haTP7JWud4R"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=dataset.vocab['<PAD>'])\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "    self.attention = nn.Linear(embed_size,embed_size)\n",
        "    self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings = self.embedding(captions)\n",
        "    combined = torch.cat((features, embeddings), dim=1)\n",
        "\n",
        "    attention_weights = torch.softmax(self.attention(combined), dim=1)\n",
        "    attention_encoding = torch.sum(combined * attention_weights, dim=1)\n",
        "\n",
        "    lstm_out, _ = self.lstm(combined)\n",
        "\n",
        "    outputs = self.fc(lstm_out)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "ymexnplm23lK"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "encoder = Encoder(embed_size).to(device)\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "decoder = Decoder(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "gUU6kEjQ-9or",
        "outputId": "cfa2aafa-d06a-4f04-894b-305fa8dc2ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Wide_ResNet50_2_Weights.IMAGENET1K_V1`. You can also use `weights=Wide_ResNet50_2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<PAD>'])\n",
        "optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "  i = 0\n",
        "  for images, captions in train_loader:\n",
        "      images = images.to(device, dtype=torch.float)\n",
        "      captions = captions.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      # captions[:, :-1]\n",
        "      features = encoder(images)\n",
        "\n",
        "      outputs = decoder(features, captions[:, :-1])\n",
        "\n",
        "      # Reshape captions for loss calculation\n",
        "      targets = captions[:, :].reshape(-1)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print the loss\n",
        "      if i % 100 == 0:\n",
        "        print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "      i+=1"
      ],
      "metadata": {
        "id": "6i_RgnkOBbYw",
        "outputId": "74d434a7-46ff-4f0e-d533-640f04f38cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/3007], Loss: 9.9193\n",
            "Epoch [1/10], Step [100/3007], Loss: 4.8602\n",
            "Epoch [1/10], Step [200/3007], Loss: 4.7621\n",
            "Epoch [1/10], Step [300/3007], Loss: 4.4860\n",
            "Epoch [1/10], Step [400/3007], Loss: 4.4923\n",
            "Epoch [1/10], Step [500/3007], Loss: 4.4511\n",
            "Epoch [1/10], Step [600/3007], Loss: 3.8655\n",
            "Epoch [1/10], Step [700/3007], Loss: 4.0662\n",
            "Epoch [1/10], Step [800/3007], Loss: 3.8946\n",
            "Epoch [1/10], Step [900/3007], Loss: 3.7423\n",
            "Epoch [1/10], Step [1000/3007], Loss: 3.6225\n",
            "Epoch [1/10], Step [1100/3007], Loss: 3.9986\n",
            "Epoch [1/10], Step [1200/3007], Loss: 3.6182\n",
            "Epoch [1/10], Step [1300/3007], Loss: 3.9326\n",
            "Epoch [1/10], Step [1400/3007], Loss: 4.0666\n",
            "Epoch [1/10], Step [1500/3007], Loss: 3.5699\n",
            "Epoch [1/10], Step [1600/3007], Loss: 3.9599\n",
            "Epoch [1/10], Step [1700/3007], Loss: 3.3994\n",
            "Epoch [1/10], Step [1800/3007], Loss: 3.4466\n",
            "Epoch [1/10], Step [1900/3007], Loss: 3.7824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(encoder.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/encoder.pt')"
      ],
      "metadata": {
        "id": "DN4xkuG0SB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(decoder.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/decoder.pt')"
      ],
      "metadata": {
        "id": "0ysTktxKSRo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        \n",
        "        # Load the pretrained ResNet-101 model\n",
        "        self.resnet = models.wide_resnet50_2(weights=None)\n",
        "        self.resnet.fc = nn.Linear(2048, embed_size)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=dataset.vocab['<PAD>'])\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.attention = nn.Linear(embed_size,embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        features = self.resnet(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        combined = torch.cat((features, embeddings), dim=1)\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(combined), dim=1)\n",
        "        attention_encoding = torch.sum(combined * attention_weights, dim=1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "\n",
        "        outputs = self.fc(lstm_out)\n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6vCRizEKHyg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256  # Size of the word embedding\n",
        "hidden_size = 512  # Size of the LSTM hidden state\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "v04TdT8KIShA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(x, y, a):\n",
        "    lam = np.random.beta(a,a)\n",
        "    rand = torch.randperm(batch_size)\n",
        "    x2 = x[rand,:]\n",
        "    y2 = y[rand,:]\n",
        "    X = lam * x + (1 - lam) * x2\n",
        "    Y = lam * y + (1 - lam) * y2\n",
        "    #print(Y)\n",
        "    return X, Y\n",
        "\n",
        "def cutout(image, k):\n",
        "    x = np.random.randint(0, 128-k)\n",
        "    y = np.random.randint(0, 128-k)\n",
        "    image[:, x:x+k, y:y+k] = 0\n",
        "    return image\n",
        "\n",
        "def standard_augmentation(image,k):\n",
        "    k1 = np.random.randint(-k, k)\n",
        "    k2 = np.random.randint(-k, k)\n",
        "    image = np.roll(image, k1, axis=1)\n",
        "    image = np.roll(image, k2, axis=2)\n",
        "    if (k1 > 0):    \n",
        "        image[:, :k1, :] = 0\n",
        "    else:\n",
        "        image[:, k1:, :] = 0\n",
        "    if (k2 > 0):\n",
        "        image[:, :, :k2] = 0\n",
        "    else:\n",
        "        image[:, :, k2:] = 0\n",
        "    if (np.random.rand() < 0.5):\n",
        "        image = np.flip(image, axis=2)\n",
        "    return image"
      ],
      "metadata": {
        "id": "4Y9TNrlpu_NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab['<PAD>'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "def training():\n",
        "  for epoch in range(num_epochs):\n",
        "      i = 0\n",
        "      copy_train_loader = deepcopy(train_loader)\n",
        "      for images, captions in copy_train_loader:\n",
        "          for j in range(len(images)):\n",
        "            if (np.random.rand() < 0.5):\n",
        "              images[j] = cutout(images[j], 64)\n",
        "            images[j] = torch.from_numpy(standard_augmentation(images[j],16).copy())\n",
        "          #images, captions = mixup(images, captions, 0.4)\n",
        "          images = images.to(device, dtype=torch.float)\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          # captions[:, :-1]\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # Reshape captions for loss calculation\n",
        "          targets = captions[:, :].reshape(-1)\n",
        "          \n",
        "          # Compute loss\n",
        "          loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print the loss\n",
        "          if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "          i+=1\n",
        "\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)\n",
        "training()"
      ],
      "metadata": {
        "id": "v3eCeIOEIjTI",
        "outputId": "a5066acb-3e67-48f2-8597-1afc056df698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/3007], Loss: 9.9184\n",
            "Epoch [1/10], Step [100/3007], Loss: 5.2330\n",
            "Epoch [1/10], Step [200/3007], Loss: 4.4731\n",
            "Epoch [1/10], Step [300/3007], Loss: 4.6244\n",
            "Epoch [1/10], Step [400/3007], Loss: 4.2619\n",
            "Epoch [1/10], Step [500/3007], Loss: 4.5868\n",
            "Epoch [1/10], Step [600/3007], Loss: 4.2361\n",
            "Epoch [1/10], Step [700/3007], Loss: 3.9252\n",
            "Epoch [1/10], Step [800/3007], Loss: 4.0310\n",
            "Epoch [1/10], Step [900/3007], Loss: 4.0650\n",
            "Epoch [1/10], Step [1000/3007], Loss: 4.2325\n",
            "Epoch [1/10], Step [1100/3007], Loss: 4.1117\n",
            "Epoch [1/10], Step [1200/3007], Loss: 3.9871\n",
            "Epoch [1/10], Step [1300/3007], Loss: 3.9052\n",
            "Epoch [1/10], Step [1400/3007], Loss: 3.5907\n",
            "Epoch [1/10], Step [1500/3007], Loss: 3.7803\n",
            "Epoch [1/10], Step [1600/3007], Loss: 3.9348\n",
            "Epoch [1/10], Step [1700/3007], Loss: 3.9470\n",
            "Epoch [1/10], Step [1800/3007], Loss: 3.4822\n",
            "Epoch [1/10], Step [1900/3007], Loss: 3.4118\n",
            "Epoch [1/10], Step [2000/3007], Loss: 3.9405\n",
            "Epoch [1/10], Step [2100/3007], Loss: 3.5416\n",
            "Epoch [1/10], Step [2200/3007], Loss: 3.7316\n",
            "Epoch [1/10], Step [2300/3007], Loss: 3.7291\n",
            "Epoch [1/10], Step [2400/3007], Loss: 3.5864\n",
            "Epoch [1/10], Step [2500/3007], Loss: 3.6843\n",
            "Epoch [1/10], Step [2600/3007], Loss: 3.5286\n",
            "Epoch [1/10], Step [2700/3007], Loss: 3.8087\n",
            "Epoch [1/10], Step [2800/3007], Loss: 3.7374\n",
            "Epoch [1/10], Step [2900/3007], Loss: 3.5462\n",
            "Epoch [1/10], Step [3000/3007], Loss: 3.6832\n",
            "Epoch [2/10], Step [0/3007], Loss: 3.6704\n",
            "Epoch [2/10], Step [100/3007], Loss: 3.2369\n",
            "Epoch [2/10], Step [200/3007], Loss: 3.7586\n",
            "Epoch [2/10], Step [300/3007], Loss: 3.3677\n",
            "Epoch [2/10], Step [400/3007], Loss: 3.1893\n",
            "Epoch [2/10], Step [500/3007], Loss: 3.4028\n",
            "Epoch [2/10], Step [600/3007], Loss: 3.2428\n",
            "Epoch [2/10], Step [700/3007], Loss: 3.4452\n",
            "Epoch [2/10], Step [800/3007], Loss: 3.3955\n",
            "Epoch [2/10], Step [900/3007], Loss: 3.3666\n",
            "Epoch [2/10], Step [1000/3007], Loss: 3.3910\n",
            "Epoch [2/10], Step [1100/3007], Loss: 3.2595\n",
            "Epoch [2/10], Step [1200/3007], Loss: 3.3798\n",
            "Epoch [2/10], Step [1300/3007], Loss: 3.2037\n",
            "Epoch [2/10], Step [1400/3007], Loss: 3.1027\n",
            "Epoch [2/10], Step [1500/3007], Loss: 3.5604\n",
            "Epoch [2/10], Step [1600/3007], Loss: 3.6426\n",
            "Epoch [2/10], Step [1700/3007], Loss: 3.3936\n",
            "Epoch [2/10], Step [1800/3007], Loss: 3.2280\n",
            "Epoch [2/10], Step [1900/3007], Loss: 3.7525\n",
            "Epoch [2/10], Step [2000/3007], Loss: 3.3976\n",
            "Epoch [2/10], Step [2100/3007], Loss: 3.3570\n",
            "Epoch [2/10], Step [2200/3007], Loss: 3.3270\n",
            "Epoch [2/10], Step [2300/3007], Loss: 3.4868\n",
            "Epoch [2/10], Step [2400/3007], Loss: 3.6040\n",
            "Epoch [2/10], Step [2500/3007], Loss: 3.2637\n",
            "Epoch [2/10], Step [2600/3007], Loss: 3.3054\n",
            "Epoch [2/10], Step [2700/3007], Loss: 3.3177\n",
            "Epoch [2/10], Step [2800/3007], Loss: 3.2919\n",
            "Epoch [2/10], Step [2900/3007], Loss: 3.1650\n",
            "Epoch [2/10], Step [3000/3007], Loss: 3.2467\n",
            "Epoch [3/10], Step [0/3007], Loss: 2.6774\n",
            "Epoch [3/10], Step [100/3007], Loss: 3.0545\n",
            "Epoch [3/10], Step [200/3007], Loss: 3.0632\n",
            "Epoch [3/10], Step [300/3007], Loss: 3.2637\n",
            "Epoch [3/10], Step [400/3007], Loss: 3.1946\n",
            "Epoch [3/10], Step [500/3007], Loss: 3.2218\n",
            "Epoch [3/10], Step [600/3007], Loss: 2.9505\n",
            "Epoch [3/10], Step [700/3007], Loss: 2.9060\n",
            "Epoch [3/10], Step [800/3007], Loss: 3.1588\n",
            "Epoch [3/10], Step [900/3007], Loss: 3.3377\n",
            "Epoch [3/10], Step [1000/3007], Loss: 3.0831\n",
            "Epoch [3/10], Step [1100/3007], Loss: 3.0215\n",
            "Epoch [3/10], Step [1200/3007], Loss: 3.1751\n",
            "Epoch [3/10], Step [1300/3007], Loss: 3.0641\n",
            "Epoch [3/10], Step [1400/3007], Loss: 3.1025\n",
            "Epoch [3/10], Step [1500/3007], Loss: 3.3414\n",
            "Epoch [3/10], Step [1600/3007], Loss: 2.9385\n",
            "Epoch [3/10], Step [1700/3007], Loss: 2.9496\n",
            "Epoch [3/10], Step [1800/3007], Loss: 2.8470\n",
            "Epoch [3/10], Step [1900/3007], Loss: 3.0729\n",
            "Epoch [3/10], Step [2000/3007], Loss: 3.2541\n",
            "Epoch [3/10], Step [2100/3007], Loss: 3.2167\n",
            "Epoch [3/10], Step [2200/3007], Loss: 3.2161\n",
            "Epoch [3/10], Step [2300/3007], Loss: 3.1452\n",
            "Epoch [3/10], Step [2400/3007], Loss: 3.2101\n",
            "Epoch [3/10], Step [2500/3007], Loss: 3.2946\n",
            "Epoch [3/10], Step [2600/3007], Loss: 3.0862\n",
            "Epoch [3/10], Step [2700/3007], Loss: 3.0642\n",
            "Epoch [3/10], Step [2800/3007], Loss: 3.1223\n",
            "Epoch [3/10], Step [2900/3007], Loss: 2.9567\n",
            "Epoch [3/10], Step [3000/3007], Loss: 3.0344\n",
            "Epoch [4/10], Step [0/3007], Loss: 2.9612\n",
            "Epoch [4/10], Step [100/3007], Loss: 2.8131\n",
            "Epoch [4/10], Step [200/3007], Loss: 2.8885\n",
            "Epoch [4/10], Step [300/3007], Loss: 2.7152\n",
            "Epoch [4/10], Step [400/3007], Loss: 2.7817\n",
            "Epoch [4/10], Step [500/3007], Loss: 3.0530\n",
            "Epoch [4/10], Step [600/3007], Loss: 2.8928\n",
            "Epoch [4/10], Step [700/3007], Loss: 2.8331\n",
            "Epoch [4/10], Step [800/3007], Loss: 2.8375\n",
            "Epoch [4/10], Step [900/3007], Loss: 2.7169\n",
            "Epoch [4/10], Step [1000/3007], Loss: 2.9755\n",
            "Epoch [4/10], Step [1100/3007], Loss: 3.0300\n",
            "Epoch [4/10], Step [1200/3007], Loss: 2.8964\n",
            "Epoch [4/10], Step [1300/3007], Loss: 2.8644\n",
            "Epoch [4/10], Step [1400/3007], Loss: 2.8638\n",
            "Epoch [4/10], Step [1500/3007], Loss: 2.9012\n",
            "Epoch [4/10], Step [1600/3007], Loss: 2.9072\n",
            "Epoch [4/10], Step [1700/3007], Loss: 2.8288\n",
            "Epoch [4/10], Step [1800/3007], Loss: 2.9152\n",
            "Epoch [4/10], Step [1900/3007], Loss: 3.0557\n",
            "Epoch [4/10], Step [2000/3007], Loss: 2.9927\n",
            "Epoch [4/10], Step [2100/3007], Loss: 2.9569\n",
            "Epoch [4/10], Step [2200/3007], Loss: 3.0967\n",
            "Epoch [4/10], Step [2300/3007], Loss: 2.9803\n",
            "Epoch [4/10], Step [2400/3007], Loss: 3.2262\n",
            "Epoch [4/10], Step [2500/3007], Loss: 2.9766\n",
            "Epoch [4/10], Step [2600/3007], Loss: 2.8617\n",
            "Epoch [4/10], Step [2700/3007], Loss: 3.0759\n",
            "Epoch [4/10], Step [2800/3007], Loss: 2.9568\n",
            "Epoch [4/10], Step [2900/3007], Loss: 2.9170\n",
            "Epoch [4/10], Step [3000/3007], Loss: 3.1705\n",
            "Epoch [5/10], Step [0/3007], Loss: 2.6336\n",
            "Epoch [5/10], Step [100/3007], Loss: 2.6385\n",
            "Epoch [5/10], Step [200/3007], Loss: 2.6919\n",
            "Epoch [5/10], Step [300/3007], Loss: 2.6710\n",
            "Epoch [5/10], Step [400/3007], Loss: 2.8370\n",
            "Epoch [5/10], Step [500/3007], Loss: 2.6878\n",
            "Epoch [5/10], Step [600/3007], Loss: 2.6522\n",
            "Epoch [5/10], Step [700/3007], Loss: 2.7015\n",
            "Epoch [5/10], Step [800/3007], Loss: 2.7658\n",
            "Epoch [5/10], Step [900/3007], Loss: 2.5868\n",
            "Epoch [5/10], Step [1000/3007], Loss: 2.7411\n",
            "Epoch [5/10], Step [1100/3007], Loss: 2.8406\n",
            "Epoch [5/10], Step [1200/3007], Loss: 2.6465\n",
            "Epoch [5/10], Step [1300/3007], Loss: 2.7369\n",
            "Epoch [5/10], Step [1400/3007], Loss: 2.7603\n",
            "Epoch [5/10], Step [1500/3007], Loss: 2.5642\n",
            "Epoch [5/10], Step [1600/3007], Loss: 2.6449\n",
            "Epoch [5/10], Step [1700/3007], Loss: 2.6616\n",
            "Epoch [5/10], Step [1800/3007], Loss: 2.8383\n",
            "Epoch [5/10], Step [1900/3007], Loss: 2.7509\n",
            "Epoch [5/10], Step [2000/3007], Loss: 2.8650\n",
            "Epoch [5/10], Step [2100/3007], Loss: 2.8598\n",
            "Epoch [5/10], Step [2200/3007], Loss: 2.8467\n",
            "Epoch [5/10], Step [2300/3007], Loss: 2.8180\n",
            "Epoch [5/10], Step [2400/3007], Loss: 2.7482\n",
            "Epoch [5/10], Step [2500/3007], Loss: 2.8535\n",
            "Epoch [5/10], Step [2600/3007], Loss: 2.6218\n",
            "Epoch [5/10], Step [2700/3007], Loss: 2.8755\n",
            "Epoch [5/10], Step [2800/3007], Loss: 2.6939\n",
            "Epoch [5/10], Step [2900/3007], Loss: 2.6532\n",
            "Epoch [5/10], Step [3000/3007], Loss: 2.7167\n",
            "Epoch [6/10], Step [0/3007], Loss: 2.5137\n",
            "Epoch [6/10], Step [100/3007], Loss: 2.7028\n",
            "Epoch [6/10], Step [200/3007], Loss: 2.3853\n",
            "Epoch [6/10], Step [300/3007], Loss: 2.4062\n",
            "Epoch [6/10], Step [400/3007], Loss: 2.6338\n",
            "Epoch [6/10], Step [500/3007], Loss: 2.6286\n",
            "Epoch [6/10], Step [600/3007], Loss: 2.5753\n",
            "Epoch [6/10], Step [700/3007], Loss: 2.6689\n",
            "Epoch [6/10], Step [800/3007], Loss: 2.6328\n",
            "Epoch [6/10], Step [900/3007], Loss: 2.4942\n",
            "Epoch [6/10], Step [1000/3007], Loss: 2.5327\n",
            "Epoch [6/10], Step [1100/3007], Loss: 2.7338\n",
            "Epoch [6/10], Step [1200/3007], Loss: 2.4251\n",
            "Epoch [6/10], Step [1300/3007], Loss: 2.7119\n",
            "Epoch [6/10], Step [1400/3007], Loss: 2.5111\n",
            "Epoch [6/10], Step [1500/3007], Loss: 2.5933\n",
            "Epoch [6/10], Step [1600/3007], Loss: 2.7298\n",
            "Epoch [6/10], Step [1700/3007], Loss: 2.4764\n",
            "Epoch [6/10], Step [1800/3007], Loss: 2.6300\n",
            "Epoch [6/10], Step [1900/3007], Loss: 2.6302\n",
            "Epoch [6/10], Step [2000/3007], Loss: 2.5185\n",
            "Epoch [6/10], Step [2100/3007], Loss: 2.5484\n",
            "Epoch [6/10], Step [2200/3007], Loss: 2.6834\n",
            "Epoch [6/10], Step [2300/3007], Loss: 2.6381\n",
            "Epoch [6/10], Step [2400/3007], Loss: 2.6976\n",
            "Epoch [6/10], Step [2500/3007], Loss: 2.5397\n",
            "Epoch [6/10], Step [2600/3007], Loss: 2.7355\n",
            "Epoch [6/10], Step [2700/3007], Loss: 2.5989\n",
            "Epoch [6/10], Step [2800/3007], Loss: 2.6211\n",
            "Epoch [6/10], Step [2900/3007], Loss: 2.5963\n",
            "Epoch [6/10], Step [3000/3007], Loss: 2.7561\n",
            "Epoch [7/10], Step [0/3007], Loss: 2.4160\n",
            "Epoch [7/10], Step [100/3007], Loss: 2.2822\n",
            "Epoch [7/10], Step [200/3007], Loss: 2.2675\n",
            "Epoch [7/10], Step [300/3007], Loss: 2.3154\n",
            "Epoch [7/10], Step [400/3007], Loss: 2.3995\n",
            "Epoch [7/10], Step [500/3007], Loss: 2.2238\n",
            "Epoch [7/10], Step [600/3007], Loss: 2.4413\n",
            "Epoch [7/10], Step [700/3007], Loss: 2.2958\n",
            "Epoch [7/10], Step [800/3007], Loss: 2.3140\n",
            "Epoch [7/10], Step [900/3007], Loss: 2.4623\n",
            "Epoch [7/10], Step [1000/3007], Loss: 2.4364\n",
            "Epoch [7/10], Step [1100/3007], Loss: 2.5560\n",
            "Epoch [7/10], Step [1200/3007], Loss: 2.4673\n",
            "Epoch [7/10], Step [1300/3007], Loss: 2.5450\n",
            "Epoch [7/10], Step [1400/3007], Loss: 2.4497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)"
      ],
      "metadata": {
        "id": "umYg9GvvPeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_image = dataset[0][0]\n",
        "image = dataset[0][0].unsqueeze(0)\n",
        "caption = dataset[0][1].unsqueeze(0)\n",
        "image = image.to(device, dtype=torch.float)\n",
        "caption = caption.to(device)"
      ],
      "metadata": {
        "id": "W_lfCl54RXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs = model(image,caption)\n",
        "  _,preds = torch.max(outputs,2)"
      ],
      "metadata": {
        "id": "fJcMPk-URYHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_preds = []\n",
        "for _,num in enumerate(preds[0]):\n",
        "  idx = num.item()\n",
        "  if idx not in dataset.vocab.lookup_indices(['<PAD>','<SOS>','<EOS>']):\n",
        "    translated_preds.append(dataset.vocab.lookup_token(idx))\n",
        "\n",
        "pred_caption = ' '.join(translated_preds)\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "unt9VMcIS9mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_caption = []\n",
        "for _,num in enumerate(caption[0]):\n",
        "  idx = num.item()\n",
        "  if idx not in dataset.vocab.lookup_indices(['<PAD>','<SOS>','<EOS>']):\n",
        "    translated_caption.append(dataset.vocab.lookup_token(idx))"
      ],
      "metadata": {
        "id": "nyuuJu-2TMIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score(translated_preds[:17],translated_caption)"
      ],
      "metadata": {
        "id": "uigTTKFLTB1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(normal_image.permute(1, 2, 0))\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "H3gNp2TbXp7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/model_UPDATED_2.pt')"
      ],
      "metadata": {
        "id": "kjt2WuekBHj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}