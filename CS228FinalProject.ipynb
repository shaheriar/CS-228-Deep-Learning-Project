{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaheriar/CS-228-Deep-Learning-Project/blob/Experimental-Changes/CS228FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 228 Final Project\n",
        "## Enhancing Image Captioning with Deep Learning Models\n",
        "### Saul Gonzalez - sgonz081\n",
        "### Shaheriar Malik - smali032\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "Image captioning is a difficult task that is one step above image classification since we are\n",
        "generating an actual text description of each image. So, deep learning would be an obvious choice in this case since generating text for a variable input image is a difficult task that would require a complex model.\n"
      ],
      "metadata": {
        "id": "jNU4I8Ai0yzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fgiadna8xoGq",
        "outputId": "992ef0ea-4ae3-4499-c945-2a22ff9df41e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n",
        "from copy import deepcopy\n",
        "\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n",
        "import os\n",
        "import natsort\n",
        "import cv2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/My Drive/ImageCaptioningDataset/flickr30k_images.zip\" -d \"/content\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "anVSuIG2PNc5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('/content/drive/MyDrive/ImageCaptioningDataset/results.csv', sep='|', header = None)\n",
        "#raw_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "#raw_df.columns = raw_df.columns.str.replace(' ', '')\n",
        "#raw_df.loc[19999,'comment'] = 'A dog runs across the grass .'\n",
        "#raw_df.to_csv('out.csv',sep='|',index=False)\n",
        "#raw_df.iloc[[19999]]"
      ],
      "metadata": {
        "id": "rGJLib9AXtS3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read, resize and zero pad images. Returns image tensor [3, 256, 256]\n",
        "def readImage(path):\n",
        "    desired_size = 256 # 256 x 256\n",
        "    image = cv2.imread(path)\n",
        "    old_size = image.shape[:2]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(new_im)\n",
        "    return tensor\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    num = temp_df.shape[0]\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(num):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n",
        "        image = readImage(f\"/content/flickr30k_images/{temp_df.image_name[i]}\")\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.comment[i], 30)))\n",
        "\n",
        "#num = 5\n",
        "#display_images(raw_df.sample(num))"
      ],
      "metadata": {
        "id": "MfB4KsAgZTrB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary for Captions\n",
        "def build_vocab(captions,tokenizer,max_len):\n",
        "  counter = Counter()\n",
        "  current_max = max_len\n",
        "  for caption in captions:\n",
        "    sentence = caption.strip()\n",
        "    tokens = tokenizer(sentence)\n",
        "    if len(tokens) > current_max:\n",
        "      current_max = len(tokens)\n",
        "    counter.update(tokens)\n",
        "  sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  dictionary = OrderedDict(sorted_by_freq)\n",
        "  dictionary = dict(dictionary)\n",
        "  dictionary['<PAD>'] = len(dictionary)\n",
        "  # dictionary['SOS'] = len(dictionary) + 1\n",
        "  # dictionary['EOS'] = len(dictionary) + 2\n",
        "  # dictionary['<UNK>'] = len(dictionary) + 3\n",
        "  dictionary = OrderedDict(dictionary)\n",
        "  return vocab(dictionary), current_max"
      ],
      "metadata": {
        "id": "E2vsl3IoFom4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_csv, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_len = -1\n",
        "\n",
        "        self.data_df = pd.read_csv(label_csv, sep='|',header=None)\n",
        "        self.data_df = self.data_df.dropna()\n",
        "        #self.data_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "        #self.data_df.columns = self.data_df.columns.str.replace(' ', '')\n",
        "        self.captions = self.data_df[1] # 0 is image_name, 1 is comment\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.data_df[0]#os.listdir(data_dir)  # Assumes images are directly under data_dir\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.vocab, self.target_len = build_vocab(self.captions.tolist(),self.tokenizer,self.target_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = (image - np.mean(image))/np.std(image) # normalize\n",
        "        image = Image.fromarray(image)\n",
        "        #image = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[idx]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Vectorize the given caption using our dataset's vocabulary\n",
        "        caption = caption.strip()\n",
        "        caption = self.tokenizer(caption)\n",
        "        caption_vector = [self.vocab[word] for word in caption]\n",
        "\n",
        "        # Add padding to the vector if it needs it\n",
        "        if len(caption_vector) < self.target_len:\n",
        "          for i in range(self.target_len - len(caption_vector)):\n",
        "            caption_vector.append(self.vocab['<PAD>'])\n",
        "\n",
        "        # Return the processed image and any associated labels\n",
        "        return image, torch.tensor(caption_vector)\n"
      ],
      "metadata": {
        "id": "pKYERZ5AUtFM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "data_dir = '/content/flickr30k_images/'\n",
        "label_csv = '/content/drive/MyDrive/ImageCaptioningDataset/results.csv'\n",
        "dataset = CustomDataset(data_dir, label_csv, transform=transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vxtZgwO6e21r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "247bd82c-eb16-4648-c7e2-58fd53dea490"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test,train = random_split(dataset, [int(len(dataset)*0.3)+1, int(len(dataset)*0.7)])\n",
        "print(len(test),len(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFtN18AqHI2",
        "outputId": "3440b484-d6e6-4e59-c065-cd2ad9a2d40c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47675 111240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = raw_df[0]\n",
        "image_paths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZGiNPk2eyir",
        "outputId": "86b4125c-8644-43f3-e720-19ed3ccecc23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000092795.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 37\n",
        "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EfbFZUHSVJ16"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# i = 0\n",
        "# for img, file_name in dataloader:\n",
        "#   clear_output()\n",
        "#   #print(label[0])\n",
        "#   #print(raw_df['image_name'])\n",
        "#   plt.imshow(img[0].permute(1,2,0))\n",
        "#   print(list(raw_df.loc[raw_df['image_name'] == file_name[0]].comment)[0])\n",
        "#   i+=5\n",
        "#   plt.show()\n",
        "#   time.sleep(1)"
      ],
      "metadata": {
        "id": "8Om2xhbrV8Kf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neual Network (CNN)"
      ],
      "metadata": {
        "id": "NwlKzpy10_cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tXi1uiPd0_XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "bkHmM37T9822",
        "outputId": "841556c4-19de-41f2-8501-1b4fcd20d453",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20217"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        \n",
        "        # Load the pretrained ResNet-101 model\n",
        "        self.resnet = models.wide_resnet50_2(weights=None)\n",
        "        self.resnet.fc = nn.Linear(2048, embed_size)\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.resnet(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        \n",
        "        embeddings = self.embedding(captions)\n",
        "        \n",
        "        combined = torch.cat((features, embeddings), dim=1)\n",
        "        \n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "        \n",
        "        outputs = self.fc(lstm_out)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6vCRizEKHyg0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256  # Size of the word embedding\n",
        "hidden_size = 512  # Size of the LSTM hidden state\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "v04TdT8KIShA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(x, y, a):\n",
        "    lam = np.random.beta(a,a)\n",
        "    rand = torch.randperm(batch_size)\n",
        "    x2 = x[rand,:]\n",
        "    y2 = y[rand,:]\n",
        "    X = lam * x + (1 - lam) * x2\n",
        "    Y = lam * y + (1 - lam) * y2\n",
        "    return X, Y\n",
        "\n",
        "def cutout(image, k):\n",
        "    x = np.random.randint(0, 128-k)\n",
        "    y = np.random.randint(0, 128-k)\n",
        "    image[:, x:x+k, y:y+k] = 0\n",
        "    return image\n",
        "\n",
        "def standard_augmentation(image,k):\n",
        "    k1 = np.random.randint(-k, k)\n",
        "    k2 = np.random.randint(-k, k)\n",
        "    image = np.roll(image, k1, axis=1)\n",
        "    image = np.roll(image, k2, axis=2)\n",
        "    if (k1 > 0):    \n",
        "        image[:, :k1, :] = 0\n",
        "    else:\n",
        "        image[:, k1:, :] = 0\n",
        "    if (k2 > 0):\n",
        "        image[:, :, :k2] = 0\n",
        "    else:\n",
        "        image[:, :, k2:] = 0\n",
        "    if (np.random.rand() < 0.5):\n",
        "        image = np.flip(image, axis=2)\n",
        "    return image"
      ],
      "metadata": {
        "id": "4Y9TNrlpu_NE"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "def training():\n",
        "  for epoch in range(num_epochs):\n",
        "      i = 0\n",
        "      copy_train_loader = deepcopy(train_loader)\n",
        "      for images, captions in copy_train_loader:\n",
        "          # Move images and captions to device (e.g., GPU)\n",
        "          #if (np.random.rand() < 0.5):\n",
        "                #image = X_epoch[j].cpu()\n",
        "                #X_epoch[j] = cutout(image, 16)\n",
        "              #image = X_epoch[j].cpu()\n",
        "              #X_epoch[j] = torch.from_numpy(standard_augmentation(image, 4).copy()).to(device)\n",
        "            #X_epoch, Y_epoch = mixup(X_epoch, Y_epoch, alpha)\n",
        "          images = images.to(device)\n",
        "          for j in range(len(images)):\n",
        "            if (np.random.rand() < 0.5):\n",
        "              images[j] = cutout(images[j], 64)\n",
        "            images[j] = torch.from_numpy(standard_augmentation(images[j].cpu(),16).copy())\n",
        "          #images, captions = mixup(images, captions, 0.4)\n",
        "\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # Reshape captions for loss calculation\n",
        "          targets = captions[:, :].reshape(-1)\n",
        "          \n",
        "          # Compute loss\n",
        "          loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print the loss\n",
        "          if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "          i+=1\n",
        "\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)\n",
        "training()"
      ],
      "metadata": {
        "id": "v3eCeIOEIjTI",
        "outputId": "e19ac0cb-f32b-4700-f28e-837a36ce76d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-be10e384c7e5>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-be10e384c7e5>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m           \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m           \u001b[0;31m# Backward pass and optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 234.00 MiB (GPU 0; 14.75 GiB total capacity; 13.32 GiB already allocated; 38.81 MiB free; 13.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_image = dataset[0][0]\n",
        "image = dataset[0][0].unsqueeze(0)\n",
        "caption = dataset[0][1].unsqueeze(0)\n",
        "image = image.to(device)\n",
        "caption = caption.to(device)"
      ],
      "metadata": {
        "id": "W_lfCl54RXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs = model(image,caption)\n",
        "  _,preds = torch.max(outputs,2)"
      ],
      "metadata": {
        "id": "fJcMPk-URYHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_preds = []\n",
        "for _,num in enumerate(preds[0]):\n",
        "  idx = num.item()\n",
        "  if idx != 20216:\n",
        "    translated_preds.append(dataset.vocab.lookup_token(idx))\n",
        "\n",
        "pred_caption = ' '.join(translated_preds)\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "unt9VMcIS9mt",
        "outputId": "9a811e58-12a5-4d11-ac09-05eb6aaeff28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a men boys are long hair are at the hands . sitting out in the background .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(normal_image.permute(1, 2, 0))\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "H3gNp2TbXp7C",
        "outputId": "84d43be9-c61f-41f3-b5a6-ee17f32b24b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-dff0c1cfbc5c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_caption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'normal_image' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}