{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shaheriar/CS-228-Deep-Learning-Project/blob/Experimental-Changes/CS228FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS 228 Final Project\n",
        "## Enhancing Image Captioning with Deep Learning Models\n",
        "### Saul Gonzalez - sgonz081\n",
        "### Shaheriar Malik - smali032\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "Image captioning is a difficult task that is one step above image classification since we are\n",
        "generating an actual text description of each image. So, deep learning would be an obvious choice in this case since generating text for a variable input image is a difficult task that would require a complex model.\n"
      ],
      "metadata": {
        "id": "jNU4I8Ai0yzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fgiadna8xoGq",
        "outputId": "6e8f1b3f-cbff-4221-cd00-8afe02499683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n",
        "from copy import deepcopy\n",
        "\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n",
        "import os\n",
        "import natsort\n",
        "import cv2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -n \"/content/drive/MyDrive/ImageCaptioningDataset/flickr30k_images.zip\" -d \"/content\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "anVSuIG2PNc5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('/content/drive/MyDrive/ImageCaptioningDataset/results.csv', sep='|', header = None)\n",
        "#raw_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "#raw_df.columns = raw_df.columns.str.replace(' ', '')\n",
        "#raw_df.loc[19999,'comment'] = 'A dog runs across the grass .'\n",
        "#raw_df.to_csv('out.csv',sep='|',index=False)\n",
        "#raw_df.iloc[[19999]]"
      ],
      "metadata": {
        "id": "rGJLib9AXtS3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read, resize and zero pad images. Returns image tensor [3, 256, 256]\n",
        "def readImage(path):\n",
        "    desired_size = 256 # 256 x 256\n",
        "    image = cv2.imread(path)\n",
        "    old_size = image.shape[:2]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(new_im)\n",
        "    return tensor\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    num = temp_df.shape[0]\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(num):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n",
        "        image = readImage(f\"/content/flickr30k_images/{temp_df.image_name[i]}\")\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.comment[i], 30)))\n",
        "\n",
        "#num = 5\n",
        "#display_images(raw_df.sample(num))"
      ],
      "metadata": {
        "id": "MfB4KsAgZTrB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary for Captions\n",
        "def build_vocab(captions,tokenizer,max_len):\n",
        "  counter = Counter()\n",
        "  current_max = max_len\n",
        "  for caption in captions:\n",
        "    sentence = caption.strip()\n",
        "    tokens = tokenizer(sentence)\n",
        "    if len(tokens) > current_max:\n",
        "      current_max = len(tokens)\n",
        "    counter.update(tokens)\n",
        "  sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  dictionary = OrderedDict(sorted_by_freq)\n",
        "  dictionary = dict(dictionary)\n",
        "  dictionary = OrderedDict(dictionary)\n",
        "  return vocab(dictionary,specials=['<PAD>','<SOS>','<EOS>','<UNK>']), current_max"
      ],
      "metadata": {
        "id": "E2vsl3IoFom4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_csv, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_len = -1\n",
        "\n",
        "        self.data_df = pd.read_csv(label_csv, sep='|',header=None)\n",
        "        self.data_df = self.data_df.dropna()\n",
        "        #self.data_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "        #self.data_df.columns = self.data_df.columns.str.replace(' ', '')\n",
        "        self.captions = self.data_df[1] # 0 is image_name, 1 is comment\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.data_df[0]#os.listdir(data_dir)  # Assumes images are directly under data_dir\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.vocab, self.target_len = build_vocab(self.captions.tolist(),self.tokenizer,self.target_len)\n",
        "        self.target_len += 2\n",
        "        self.vocab.set_default_index(self.vocab['<UNK>'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_dir, self.image_paths[idx])\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        #image = (image - np.mean(image))/np.std(image) # normalize\n",
        "        image = Image.fromarray(image)# * 255).astype(np.uint8))\n",
        "        # = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[idx]\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Vectorize the given caption using our dataset's vocabulary\n",
        "        caption = caption.strip()\n",
        "        caption = self.tokenizer(caption)\n",
        "        caption_vector = [self.vocab['<SOS>']]\n",
        "        caption_vector.extend([self.vocab[word] for word in caption])\n",
        "        caption_vector.append(self.vocab['<EOS>'])\n",
        "\n",
        "        # Add padding to the vector if it needs it\n",
        "        if len(caption_vector) < self.target_len:\n",
        "          for i in range(self.target_len - len(caption_vector)):\n",
        "            caption_vector.append(self.vocab['<PAD>'])\n",
        "\n",
        "        # Return the processed image and any associated labels\n",
        "        return image, torch.tensor(caption_vector)"
      ],
      "metadata": {
        "id": "pKYERZ5AUtFM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0,0,0),(1,1,1))\n",
        "])\n",
        "\n",
        "data_dir = '/content/flickr30k_images/'\n",
        "label_csv = '/content/drive/MyDrive/ImageCaptioningDataset/results.csv'\n",
        "dataset = CustomDataset(data_dir, label_csv, transform=transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vxtZgwO6e21r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "308fe576-388d-4789-a546-73fa48aa7f52"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test,train = random_split(dataset, [int(len(dataset)*0.3)+1, int(len(dataset)*0.7)])\n",
        "print(len(test),len(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFtN18AqHI2",
        "outputId": "239007e2-3440-409c-cec9-ecf6597bf4be"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47675 111240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = raw_df[0]\n",
        "image_paths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZGiNPk2eyir",
        "outputId": "1c3e43db-e329-431e-937e-98d8b646149f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000092795.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 37\n",
        "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "EfbFZUHSVJ16"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# i = 0\n",
        "# for img, file_name in dataloader:\n",
        "#   clear_output()\n",
        "#   #print(label[0])\n",
        "#   #print(raw_df['image_name'])\n",
        "#   plt.imshow(img[0].permute(1,2,0))\n",
        "#   print(list(raw_df.loc[raw_df['image_name'] == file_name[0]].comment)[0])\n",
        "#   i+=5\n",
        "#   plt.show()\n",
        "#   time.sleep(1)"
      ],
      "metadata": {
        "id": "8Om2xhbrV8Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neual Network (CNN)"
      ],
      "metadata": {
        "id": "NwlKzpy10_cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tXi1uiPd0_XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "bkHmM37T9822",
        "outputId": "289466f6-2a59-4c72-d358-0f5d11db65ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20220"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        \n",
        "        # Load the pretrained ResNet-101 model\n",
        "        self.resnet = models.wide_resnet50_2(weights=None)\n",
        "        self.resnet.fc = nn.Linear(2048, embed_size)\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=dataset.vocab.lookup_indices([''])[0])\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.attention = nn.Linear(embed_size,embed_size)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "\n",
        "        features = self.resnet(images)\n",
        "        features = features.unsqueeze(1)\n",
        "\n",
        "        embeddings = self.embedding(captions)\n",
        "\n",
        "        combined = torch.cat((features, embeddings), dim=1)\n",
        "\n",
        "        attention_weights = torch.softmax(self.attention(combined), dim=1)\n",
        "        attention_encoding = torch.sum(combined * attention_weights, dim=1)\n",
        "\n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "\n",
        "        outputs = self.fc(lstm_out)\n",
        "\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6vCRizEKHyg0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256  # Size of the word embedding\n",
        "hidden_size = 512  # Size of the LSTM hidden state\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "v04TdT8KIShA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(x, y, a):\n",
        "    lam = np.random.beta(a,a)\n",
        "    rand = torch.randperm(batch_size)\n",
        "    x2 = x[rand,:]\n",
        "    y2 = y[rand,:]\n",
        "    X = lam * x + (1 - lam) * x2\n",
        "    Y = lam * y + (1 - lam) * y2\n",
        "    #print(Y)\n",
        "    return X, Y\n",
        "\n",
        "def cutout(image, k):\n",
        "    x = np.random.randint(0, 128-k)\n",
        "    y = np.random.randint(0, 128-k)\n",
        "    image[:, x:x+k, y:y+k] = 0\n",
        "    return image\n",
        "\n",
        "def standard_augmentation(image,k):\n",
        "    k1 = np.random.randint(-k, k)\n",
        "    k2 = np.random.randint(-k, k)\n",
        "    image = np.roll(image, k1, axis=1)\n",
        "    image = np.roll(image, k2, axis=2)\n",
        "    if (k1 > 0):    \n",
        "        image[:, :k1, :] = 0\n",
        "    else:\n",
        "        image[:, k1:, :] = 0\n",
        "    if (k2 > 0):\n",
        "        image[:, :, :k2] = 0\n",
        "    else:\n",
        "        image[:, :, k2:] = 0\n",
        "    if (np.random.rand() < 0.5):\n",
        "        image = np.flip(image, axis=2)\n",
        "    return image"
      ],
      "metadata": {
        "id": "4Y9TNrlpu_NE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "def training():\n",
        "  for epoch in range(num_epochs):\n",
        "      i = 0\n",
        "      copy_train_loader = deepcopy(train_loader)\n",
        "      for images, captions in copy_train_loader:\n",
        "          for j in range(len(images)):\n",
        "            if (np.random.rand() < 0.5):\n",
        "              images[j] = cutout(images[j], 64)\n",
        "            images[j] = torch.from_numpy(standard_augmentation(images[j],16).copy())\n",
        "          #images, captions = mixup(images, captions, 0.4)\n",
        "          images = images.to(device, dtype=torch.float)\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          # captions[:, :-1]\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # Reshape captions for loss calculation\n",
        "          targets = captions[:, :].reshape(-1)\n",
        "          \n",
        "          # Compute loss\n",
        "          loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print the loss\n",
        "          if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "          i+=1\n",
        "\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)\n",
        "training()"
      ],
      "metadata": {
        "id": "v3eCeIOEIjTI",
        "outputId": "16f8e99c-9f64-4e18-ec8e-5e752a0c57fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/3007], Loss: 9.7829\n",
            "Epoch [1/10], Step [100/3007], Loss: 0.8580\n",
            "Epoch [1/10], Step [200/3007], Loss: 0.8039\n",
            "Epoch [1/10], Step [300/3007], Loss: 0.6797\n",
            "Epoch [1/10], Step [400/3007], Loss: 0.7317\n",
            "Epoch [1/10], Step [500/3007], Loss: 0.6103\n",
            "Epoch [1/10], Step [600/3007], Loss: 0.7103\n",
            "Epoch [1/10], Step [700/3007], Loss: 0.7690\n",
            "Epoch [1/10], Step [800/3007], Loss: 0.6894\n",
            "Epoch [1/10], Step [900/3007], Loss: 0.6146\n",
            "Epoch [1/10], Step [1000/3007], Loss: 0.7026\n",
            "Epoch [1/10], Step [1100/3007], Loss: 0.5926\n",
            "Epoch [1/10], Step [1200/3007], Loss: 0.6541\n",
            "Epoch [1/10], Step [1300/3007], Loss: 0.6328\n",
            "Epoch [1/10], Step [1400/3007], Loss: 0.4827\n",
            "Epoch [1/10], Step [1500/3007], Loss: 0.6543\n",
            "Epoch [1/10], Step [1600/3007], Loss: 0.5449\n",
            "Epoch [1/10], Step [1700/3007], Loss: 0.5203\n",
            "Epoch [1/10], Step [1800/3007], Loss: 0.6528\n",
            "Epoch [1/10], Step [1900/3007], Loss: 0.6081\n",
            "Epoch [1/10], Step [2000/3007], Loss: 0.7109\n",
            "Epoch [1/10], Step [2100/3007], Loss: 0.6468\n",
            "Epoch [1/10], Step [2200/3007], Loss: 0.5330\n",
            "Epoch [1/10], Step [2300/3007], Loss: 0.5989\n",
            "Epoch [1/10], Step [2400/3007], Loss: 0.6136\n",
            "Epoch [1/10], Step [2500/3007], Loss: 0.5859\n",
            "Epoch [1/10], Step [2600/3007], Loss: 0.5632\n",
            "Epoch [1/10], Step [2700/3007], Loss: 0.4866\n",
            "Epoch [1/10], Step [2800/3007], Loss: 0.5529\n",
            "Epoch [1/10], Step [2900/3007], Loss: 0.5320\n",
            "Epoch [1/10], Step [3000/3007], Loss: 0.5551\n",
            "Epoch [2/10], Step [0/3007], Loss: 0.5028\n",
            "Epoch [2/10], Step [100/3007], Loss: 0.5421\n",
            "Epoch [2/10], Step [200/3007], Loss: 0.5638\n",
            "Epoch [2/10], Step [300/3007], Loss: 0.6167\n",
            "Epoch [2/10], Step [400/3007], Loss: 0.4565\n",
            "Epoch [2/10], Step [500/3007], Loss: 0.6523\n",
            "Epoch [2/10], Step [600/3007], Loss: 0.6219\n",
            "Epoch [2/10], Step [700/3007], Loss: 0.5837\n",
            "Epoch [2/10], Step [800/3007], Loss: 0.5152\n",
            "Epoch [2/10], Step [900/3007], Loss: 0.5717\n",
            "Epoch [2/10], Step [1000/3007], Loss: 0.4656\n",
            "Epoch [2/10], Step [1100/3007], Loss: 0.5130\n",
            "Epoch [2/10], Step [1200/3007], Loss: 0.5190\n",
            "Epoch [2/10], Step [1300/3007], Loss: 0.5375\n",
            "Epoch [2/10], Step [1400/3007], Loss: 0.5066\n",
            "Epoch [2/10], Step [1500/3007], Loss: 0.5212\n",
            "Epoch [2/10], Step [1600/3007], Loss: 0.4581\n",
            "Epoch [2/10], Step [1700/3007], Loss: 0.5324\n",
            "Epoch [2/10], Step [1800/3007], Loss: 0.6129\n",
            "Epoch [2/10], Step [1900/3007], Loss: 0.5017\n",
            "Epoch [2/10], Step [2000/3007], Loss: 0.5369\n",
            "Epoch [2/10], Step [2100/3007], Loss: 0.5137\n",
            "Epoch [2/10], Step [2200/3007], Loss: 0.5025\n",
            "Epoch [2/10], Step [2300/3007], Loss: 0.5572\n",
            "Epoch [2/10], Step [2400/3007], Loss: 0.5558\n",
            "Epoch [2/10], Step [2500/3007], Loss: 0.5088\n",
            "Epoch [2/10], Step [2600/3007], Loss: 0.4249\n",
            "Epoch [2/10], Step [2700/3007], Loss: 0.5303\n",
            "Epoch [2/10], Step [2800/3007], Loss: 0.5689\n",
            "Epoch [2/10], Step [2900/3007], Loss: 0.4996\n",
            "Epoch [2/10], Step [3000/3007], Loss: 0.4541\n",
            "Epoch [3/10], Step [0/3007], Loss: 0.4780\n",
            "Epoch [3/10], Step [100/3007], Loss: 0.4729\n",
            "Epoch [3/10], Step [200/3007], Loss: 0.4843\n",
            "Epoch [3/10], Step [300/3007], Loss: 0.4433\n",
            "Epoch [3/10], Step [400/3007], Loss: 0.5540\n",
            "Epoch [3/10], Step [500/3007], Loss: 0.5419\n",
            "Epoch [3/10], Step [600/3007], Loss: 0.5203\n",
            "Epoch [3/10], Step [700/3007], Loss: 0.4993\n",
            "Epoch [3/10], Step [800/3007], Loss: 0.5118\n",
            "Epoch [3/10], Step [900/3007], Loss: 0.5327\n",
            "Epoch [3/10], Step [1000/3007], Loss: 0.4436\n",
            "Epoch [3/10], Step [1100/3007], Loss: 0.4963\n",
            "Epoch [3/10], Step [1200/3007], Loss: 0.5041\n",
            "Epoch [3/10], Step [1300/3007], Loss: 0.4377\n",
            "Epoch [3/10], Step [1400/3007], Loss: 0.4816\n",
            "Epoch [3/10], Step [1500/3007], Loss: 0.4314\n",
            "Epoch [3/10], Step [1600/3007], Loss: 0.4594\n",
            "Epoch [3/10], Step [1700/3007], Loss: 0.4633\n",
            "Epoch [3/10], Step [1800/3007], Loss: 0.4804\n",
            "Epoch [3/10], Step [1900/3007], Loss: 0.4987\n",
            "Epoch [3/10], Step [2000/3007], Loss: 0.5002\n",
            "Epoch [3/10], Step [2100/3007], Loss: 0.5012\n",
            "Epoch [3/10], Step [2200/3007], Loss: 0.4406\n",
            "Epoch [3/10], Step [2300/3007], Loss: 0.4477\n",
            "Epoch [3/10], Step [2400/3007], Loss: 0.5165\n",
            "Epoch [3/10], Step [2500/3007], Loss: 0.4585\n",
            "Epoch [3/10], Step [2600/3007], Loss: 0.4479\n",
            "Epoch [3/10], Step [2700/3007], Loss: 0.5271\n",
            "Epoch [3/10], Step [2800/3007], Loss: 0.4488\n",
            "Epoch [3/10], Step [2900/3007], Loss: 0.4949\n",
            "Epoch [3/10], Step [3000/3007], Loss: 0.5392\n",
            "Epoch [4/10], Step [0/3007], Loss: 0.4565\n",
            "Epoch [4/10], Step [100/3007], Loss: 0.3769\n",
            "Epoch [4/10], Step [200/3007], Loss: 0.4300\n",
            "Epoch [4/10], Step [300/3007], Loss: 0.4609\n",
            "Epoch [4/10], Step [400/3007], Loss: 0.4873\n",
            "Epoch [4/10], Step [500/3007], Loss: 0.4591\n",
            "Epoch [4/10], Step [600/3007], Loss: 0.4327\n",
            "Epoch [4/10], Step [700/3007], Loss: 0.5113\n",
            "Epoch [4/10], Step [800/3007], Loss: 0.4358\n",
            "Epoch [4/10], Step [900/3007], Loss: 0.4119\n",
            "Epoch [4/10], Step [1000/3007], Loss: 0.4393\n",
            "Epoch [4/10], Step [1100/3007], Loss: 0.4482\n",
            "Epoch [4/10], Step [1200/3007], Loss: 0.4929\n",
            "Epoch [4/10], Step [1300/3007], Loss: 0.5116\n",
            "Epoch [4/10], Step [1400/3007], Loss: 0.4255\n",
            "Epoch [4/10], Step [1500/3007], Loss: 0.4555\n",
            "Epoch [4/10], Step [1600/3007], Loss: 0.4956\n",
            "Epoch [4/10], Step [1700/3007], Loss: 0.4827\n",
            "Epoch [4/10], Step [1800/3007], Loss: 0.4452\n",
            "Epoch [4/10], Step [1900/3007], Loss: 0.4546\n",
            "Epoch [4/10], Step [2000/3007], Loss: 0.4063\n",
            "Epoch [4/10], Step [2100/3007], Loss: 0.4929\n",
            "Epoch [4/10], Step [2200/3007], Loss: 0.4534\n",
            "Epoch [4/10], Step [2300/3007], Loss: 0.4827\n",
            "Epoch [4/10], Step [2400/3007], Loss: 0.4272\n",
            "Epoch [4/10], Step [2500/3007], Loss: 0.4537\n",
            "Epoch [4/10], Step [2600/3007], Loss: 0.4772\n",
            "Epoch [4/10], Step [2700/3007], Loss: 0.4732\n",
            "Epoch [4/10], Step [2800/3007], Loss: 0.4633\n",
            "Epoch [4/10], Step [2900/3007], Loss: 0.4827\n",
            "Epoch [4/10], Step [3000/3007], Loss: 0.5120\n",
            "Epoch [5/10], Step [0/3007], Loss: 0.4009\n",
            "Epoch [5/10], Step [100/3007], Loss: 0.3598\n",
            "Epoch [5/10], Step [200/3007], Loss: 0.4042\n",
            "Epoch [5/10], Step [300/3007], Loss: 0.4400\n",
            "Epoch [5/10], Step [400/3007], Loss: 0.3842\n",
            "Epoch [5/10], Step [500/3007], Loss: 0.4279\n",
            "Epoch [5/10], Step [600/3007], Loss: 0.4293\n",
            "Epoch [5/10], Step [700/3007], Loss: 0.4256\n",
            "Epoch [5/10], Step [800/3007], Loss: 0.3990\n",
            "Epoch [5/10], Step [900/3007], Loss: 0.4403\n",
            "Epoch [5/10], Step [1000/3007], Loss: 0.4336\n",
            "Epoch [5/10], Step [1100/3007], Loss: 0.3954\n",
            "Epoch [5/10], Step [1200/3007], Loss: 0.3909\n",
            "Epoch [5/10], Step [1300/3007], Loss: 0.4839\n",
            "Epoch [5/10], Step [1400/3007], Loss: 0.4106\n",
            "Epoch [5/10], Step [1500/3007], Loss: 0.3850\n",
            "Epoch [5/10], Step [1600/3007], Loss: 0.4831\n",
            "Epoch [5/10], Step [1700/3007], Loss: 0.4259\n",
            "Epoch [5/10], Step [1800/3007], Loss: 0.4231\n",
            "Epoch [5/10], Step [1900/3007], Loss: 0.4229\n",
            "Epoch [5/10], Step [2000/3007], Loss: 0.4834\n",
            "Epoch [5/10], Step [2100/3007], Loss: 0.4337\n",
            "Epoch [5/10], Step [2200/3007], Loss: 0.4582\n",
            "Epoch [5/10], Step [2300/3007], Loss: 0.4372\n",
            "Epoch [5/10], Step [2400/3007], Loss: 0.4372\n",
            "Epoch [5/10], Step [2500/3007], Loss: 0.4556\n",
            "Epoch [5/10], Step [2600/3007], Loss: 0.4487\n",
            "Epoch [5/10], Step [2700/3007], Loss: 0.4329\n",
            "Epoch [5/10], Step [2800/3007], Loss: 0.4101\n",
            "Epoch [5/10], Step [2900/3007], Loss: 0.4978\n",
            "Epoch [5/10], Step [3000/3007], Loss: 0.4444\n",
            "Epoch [6/10], Step [0/3007], Loss: 0.4155\n",
            "Epoch [6/10], Step [100/3007], Loss: 0.3416\n",
            "Epoch [6/10], Step [200/3007], Loss: 0.3570\n",
            "Epoch [6/10], Step [300/3007], Loss: 0.3979\n",
            "Epoch [6/10], Step [400/3007], Loss: 0.4437\n",
            "Epoch [6/10], Step [500/3007], Loss: 0.4128\n",
            "Epoch [6/10], Step [600/3007], Loss: 0.3943\n",
            "Epoch [6/10], Step [700/3007], Loss: 0.3781\n",
            "Epoch [6/10], Step [800/3007], Loss: 0.3999\n",
            "Epoch [6/10], Step [900/3007], Loss: 0.4441\n",
            "Epoch [6/10], Step [1000/3007], Loss: 0.3903\n",
            "Epoch [6/10], Step [1100/3007], Loss: 0.4267\n",
            "Epoch [6/10], Step [1200/3007], Loss: 0.4005\n",
            "Epoch [6/10], Step [1300/3007], Loss: 0.4075\n",
            "Epoch [6/10], Step [1400/3007], Loss: 0.4451\n",
            "Epoch [6/10], Step [1500/3007], Loss: 0.4431\n",
            "Epoch [6/10], Step [1600/3007], Loss: 0.3882\n",
            "Epoch [6/10], Step [1700/3007], Loss: 0.4234\n",
            "Epoch [6/10], Step [1800/3007], Loss: 0.4349\n",
            "Epoch [6/10], Step [1900/3007], Loss: 0.4307\n",
            "Epoch [6/10], Step [2000/3007], Loss: 0.5088\n",
            "Epoch [6/10], Step [2100/3007], Loss: 0.3689\n",
            "Epoch [6/10], Step [2200/3007], Loss: 0.4330\n",
            "Epoch [6/10], Step [2300/3007], Loss: 0.3804\n",
            "Epoch [6/10], Step [2400/3007], Loss: 0.3515\n",
            "Epoch [6/10], Step [2500/3007], Loss: 0.4062\n",
            "Epoch [6/10], Step [2600/3007], Loss: 0.4196\n",
            "Epoch [6/10], Step [2700/3007], Loss: 0.4513\n",
            "Epoch [6/10], Step [2800/3007], Loss: 0.4258\n",
            "Epoch [6/10], Step [2900/3007], Loss: 0.4378\n",
            "Epoch [6/10], Step [3000/3007], Loss: 0.3932\n",
            "Epoch [7/10], Step [0/3007], Loss: 0.3754\n",
            "Epoch [7/10], Step [100/3007], Loss: 0.4057\n",
            "Epoch [7/10], Step [200/3007], Loss: 0.3579\n",
            "Epoch [7/10], Step [300/3007], Loss: 0.3884\n",
            "Epoch [7/10], Step [400/3007], Loss: 0.4016\n",
            "Epoch [7/10], Step [500/3007], Loss: 0.3851\n",
            "Epoch [7/10], Step [600/3007], Loss: 0.3949\n",
            "Epoch [7/10], Step [700/3007], Loss: 0.3905\n",
            "Epoch [7/10], Step [800/3007], Loss: 0.3847\n",
            "Epoch [7/10], Step [900/3007], Loss: 0.3374\n",
            "Epoch [7/10], Step [1000/3007], Loss: 0.3751\n",
            "Epoch [7/10], Step [1100/3007], Loss: 0.3565\n",
            "Epoch [7/10], Step [1200/3007], Loss: 0.4091\n",
            "Epoch [7/10], Step [1300/3007], Loss: 0.3849\n",
            "Epoch [7/10], Step [1400/3007], Loss: 0.3959\n",
            "Epoch [7/10], Step [1500/3007], Loss: 0.4238\n",
            "Epoch [7/10], Step [1600/3007], Loss: 0.4066\n",
            "Epoch [7/10], Step [1700/3007], Loss: 0.3883\n",
            "Epoch [7/10], Step [1800/3007], Loss: 0.3986\n",
            "Epoch [7/10], Step [1900/3007], Loss: 0.4158\n",
            "Epoch [7/10], Step [2000/3007], Loss: 0.3705\n",
            "Epoch [7/10], Step [2100/3007], Loss: 0.3515\n",
            "Epoch [7/10], Step [2200/3007], Loss: 0.3832\n",
            "Epoch [7/10], Step [2300/3007], Loss: 0.3749\n",
            "Epoch [7/10], Step [2400/3007], Loss: 0.3783\n",
            "Epoch [7/10], Step [2500/3007], Loss: 0.3956\n",
            "Epoch [7/10], Step [2600/3007], Loss: 0.3374\n",
            "Epoch [7/10], Step [2700/3007], Loss: 0.3604\n",
            "Epoch [7/10], Step [2800/3007], Loss: 0.4042\n",
            "Epoch [7/10], Step [2900/3007], Loss: 0.3892\n",
            "Epoch [7/10], Step [3000/3007], Loss: 0.3848\n",
            "Epoch [8/10], Step [0/3007], Loss: 0.3468\n",
            "Epoch [8/10], Step [100/3007], Loss: 0.3768\n",
            "Epoch [8/10], Step [200/3007], Loss: 0.3617\n",
            "Epoch [8/10], Step [300/3007], Loss: 0.3427\n",
            "Epoch [8/10], Step [400/3007], Loss: 0.3927\n",
            "Epoch [8/10], Step [500/3007], Loss: 0.3333\n",
            "Epoch [8/10], Step [600/3007], Loss: 0.3754\n",
            "Epoch [8/10], Step [700/3007], Loss: 0.3744\n",
            "Epoch [8/10], Step [800/3007], Loss: 0.3371\n",
            "Epoch [8/10], Step [900/3007], Loss: 0.3443\n",
            "Epoch [8/10], Step [1000/3007], Loss: 0.3665\n",
            "Epoch [8/10], Step [1100/3007], Loss: 0.4331\n",
            "Epoch [8/10], Step [1200/3007], Loss: 0.3540\n",
            "Epoch [8/10], Step [1300/3007], Loss: 0.3769\n",
            "Epoch [8/10], Step [1400/3007], Loss: 0.3884\n",
            "Epoch [8/10], Step [1500/3007], Loss: 0.3775\n",
            "Epoch [8/10], Step [1600/3007], Loss: 0.3511\n",
            "Epoch [8/10], Step [1700/3007], Loss: 0.3605\n",
            "Epoch [8/10], Step [1800/3007], Loss: 0.3616\n",
            "Epoch [8/10], Step [1900/3007], Loss: 0.3770\n",
            "Epoch [8/10], Step [2000/3007], Loss: 0.3775\n",
            "Epoch [8/10], Step [2100/3007], Loss: 0.4009\n",
            "Epoch [8/10], Step [2200/3007], Loss: 0.3628\n",
            "Epoch [8/10], Step [2300/3007], Loss: 0.3654\n",
            "Epoch [8/10], Step [2400/3007], Loss: 0.4227\n",
            "Epoch [8/10], Step [2500/3007], Loss: 0.3497\n",
            "Epoch [8/10], Step [2600/3007], Loss: 0.3551\n",
            "Epoch [8/10], Step [2700/3007], Loss: 0.3769\n",
            "Epoch [8/10], Step [2800/3007], Loss: 0.3926\n",
            "Epoch [8/10], Step [2900/3007], Loss: 0.3979\n",
            "Epoch [8/10], Step [3000/3007], Loss: 0.4134\n",
            "Epoch [9/10], Step [0/3007], Loss: 0.3590\n",
            "Epoch [9/10], Step [100/3007], Loss: 0.3101\n",
            "Epoch [9/10], Step [200/3007], Loss: 0.3597\n",
            "Epoch [9/10], Step [300/3007], Loss: 0.3441\n",
            "Epoch [9/10], Step [400/3007], Loss: 0.3763\n",
            "Epoch [9/10], Step [500/3007], Loss: 0.3404\n",
            "Epoch [9/10], Step [600/3007], Loss: 0.3236\n",
            "Epoch [9/10], Step [700/3007], Loss: 0.3508\n",
            "Epoch [9/10], Step [800/3007], Loss: 0.3558\n",
            "Epoch [9/10], Step [900/3007], Loss: 0.3456\n",
            "Epoch [9/10], Step [1000/3007], Loss: 0.3876\n",
            "Epoch [9/10], Step [1100/3007], Loss: 0.3665\n",
            "Epoch [9/10], Step [1200/3007], Loss: 0.3675\n",
            "Epoch [9/10], Step [1300/3007], Loss: 0.3875\n",
            "Epoch [9/10], Step [1400/3007], Loss: 0.3606\n",
            "Epoch [9/10], Step [1500/3007], Loss: 0.3588\n",
            "Epoch [9/10], Step [1600/3007], Loss: 0.3743\n",
            "Epoch [9/10], Step [1700/3007], Loss: 0.3783\n",
            "Epoch [9/10], Step [1800/3007], Loss: 0.3664\n",
            "Epoch [9/10], Step [1900/3007], Loss: 0.3992\n",
            "Epoch [9/10], Step [2000/3007], Loss: 0.4219\n",
            "Epoch [9/10], Step [2100/3007], Loss: 0.3745\n",
            "Epoch [9/10], Step [2200/3007], Loss: 0.3921\n",
            "Epoch [9/10], Step [2300/3007], Loss: 0.3476\n",
            "Epoch [9/10], Step [2400/3007], Loss: 0.3746\n",
            "Epoch [9/10], Step [2500/3007], Loss: 0.3591\n",
            "Epoch [9/10], Step [2600/3007], Loss: 0.3664\n",
            "Epoch [9/10], Step [2700/3007], Loss: 0.3768\n",
            "Epoch [9/10], Step [2800/3007], Loss: 0.3541\n",
            "Epoch [9/10], Step [2900/3007], Loss: 0.3372\n",
            "Epoch [9/10], Step [3000/3007], Loss: 0.4037\n",
            "Epoch [10/10], Step [0/3007], Loss: 0.3206\n",
            "Epoch [10/10], Step [100/3007], Loss: 0.3433\n",
            "Epoch [10/10], Step [200/3007], Loss: 0.3100\n",
            "Epoch [10/10], Step [300/3007], Loss: 0.3884\n",
            "Epoch [10/10], Step [400/3007], Loss: 0.3759\n",
            "Epoch [10/10], Step [500/3007], Loss: 0.3396\n",
            "Epoch [10/10], Step [600/3007], Loss: 0.3277\n",
            "Epoch [10/10], Step [700/3007], Loss: 0.3273\n",
            "Epoch [10/10], Step [800/3007], Loss: 0.3516\n",
            "Epoch [10/10], Step [900/3007], Loss: 0.3411\n",
            "Epoch [10/10], Step [1000/3007], Loss: 0.3536\n",
            "Epoch [10/10], Step [1100/3007], Loss: 0.3346\n",
            "Epoch [10/10], Step [1200/3007], Loss: 0.3236\n",
            "Epoch [10/10], Step [1300/3007], Loss: 0.3731\n",
            "Epoch [10/10], Step [1400/3007], Loss: 0.3362\n",
            "Epoch [10/10], Step [1500/3007], Loss: 0.3303\n",
            "Epoch [10/10], Step [1600/3007], Loss: 0.3190\n",
            "Epoch [10/10], Step [1700/3007], Loss: 0.3366\n",
            "Epoch [10/10], Step [1800/3007], Loss: 0.3692\n",
            "Epoch [10/10], Step [1900/3007], Loss: 0.3335\n",
            "Epoch [10/10], Step [2000/3007], Loss: 0.3246\n",
            "Epoch [10/10], Step [2100/3007], Loss: 0.3929\n",
            "Epoch [10/10], Step [2200/3007], Loss: 0.3343\n",
            "Epoch [10/10], Step [2300/3007], Loss: 0.3270\n",
            "Epoch [10/10], Step [2400/3007], Loss: 0.3661\n",
            "Epoch [10/10], Step [2500/3007], Loss: 0.3362\n",
            "Epoch [10/10], Step [2600/3007], Loss: 0.3775\n",
            "Epoch [10/10], Step [2700/3007], Loss: 0.3776\n",
            "Epoch [10/10], Step [2800/3007], Loss: 0.3220\n",
            "Epoch [10/10], Step [2900/3007], Loss: 0.3753\n",
            "Epoch [10/10], Step [3000/3007], Loss: 0.3811\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)"
      ],
      "metadata": {
        "id": "umYg9GvvPeGr",
        "outputId": "29f340e6-1eec-4b2f-ae93-45401a522127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f127d6b40933>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1172\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             \u001b[0mtyped_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    167\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_image = dataset[0][0]\n",
        "image = dataset[0][0].unsqueeze(0)\n",
        "caption = dataset[0][1].unsqueeze(0)\n",
        "image = image.to(device, dtype=torch.float)\n",
        "caption = caption.to(device)"
      ],
      "metadata": {
        "id": "W_lfCl54RXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs = model(image,caption)\n",
        "  _,preds = torch.max(outputs,2)"
      ],
      "metadata": {
        "id": "fJcMPk-URYHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_preds = []\n",
        "for _,num in enumerate(preds[0]):\n",
        "  idx = num.item()\n",
        "  if idx != dataset.vocab.lookup_indices(['<PAD>'])[0] and idx != dataset.vocab.lookup_indices(['<SOS>'])[0] and idx != dataset.vocab.lookup_indices(['<EOS>'])[0]:\n",
        "    translated_preds.append(dataset.vocab.lookup_token(idx))\n",
        "\n",
        "pred_caption = ' '.join(translated_preds)\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "unt9VMcIS9mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(normal_image.permute(1, 2, 0))\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "H3gNp2TbXp7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.save(model.state_dict(), '/content/drive/MyDrive/ImageCaptioningDataset/model_UPDATED.pt')"
      ],
      "metadata": {
        "id": "kjt2WuekBHj9"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}