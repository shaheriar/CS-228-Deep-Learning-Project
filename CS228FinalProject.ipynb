{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 228 Final Project\n",
        "## Enhancing Image Captioning with Deep Learning Models\n",
        "### Saul Gonzalez - sgonz081\n",
        "### Shaheriar Malik - smali032\n",
        "\n",
        "Dataset: https://www.kaggle.com/datasets/hsankesara/flickr-image-dataset\n",
        "\n",
        "Image captioning is a difficult task that is one step above image classification since we are\n",
        "generating an actual text description of each image. So, deep learning would be an obvious choice in this case since generating text for a variable input image is a difficult task that would require a complex model.\n"
      ],
      "metadata": {
        "id": "jNU4I8Ai0yzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "fgiadna8xoGq",
        "outputId": "98db8fd9-e212-4e14-9c91-e6c9e6b242cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import vocab\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import random_split\n",
        "from copy import deepcopy\n",
        "\n",
        "from PIL import Image\n",
        "import nltk\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import random\n",
        "import os\n",
        "import natsort\n",
        "import cv2\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "from textwrap import wrap\n",
        "from IPython.display import clear_output\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip \"/content/drive/My Drive/ImageCaptioningDataset/flickr30k_images.zip\" -d \"/content\"\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "anVSuIG2PNc5"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('/content/drive/MyDrive/ImageCaptioningDataset/results.csv', sep='|', header = None)\n",
        "#raw_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "#raw_df.columns = raw_df.columns.str.replace(' ', '')\n",
        "#raw_df.loc[19999,'comment'] = 'A dog runs across the grass .'\n",
        "#raw_df.to_csv('out.csv',sep='|',index=False)\n",
        "#raw_df.iloc[[19999]]"
      ],
      "metadata": {
        "id": "rGJLib9AXtS3"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read, resize and zero pad images. Returns image tensor [3, 256, 256]\n",
        "def readImage(path):\n",
        "    desired_size = 256 # 256 x 256\n",
        "    image = cv2.imread(path)\n",
        "    old_size = image.shape[:2]\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    ratio = float(desired_size)/max(old_size)\n",
        "    new_size = tuple([int(x*ratio) for x in old_size])\n",
        "    image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "    delta_w = desired_size - new_size[1]\n",
        "    delta_h = desired_size - new_size[0]\n",
        "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "    new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    transform = transforms.ToTensor()\n",
        "    tensor = transform(new_im)\n",
        "    return tensor\n",
        "\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    num = temp_df.shape[0]\n",
        "    plt.figure(figsize = (20 , 20))\n",
        "    n = 0\n",
        "    for i in range(num):\n",
        "        n+=1\n",
        "        plt.subplot(5 , 5, n)\n",
        "        plt.subplots_adjust(hspace = 0.1, wspace = 0.1)\n",
        "        image = readImage(f\"/content/flickr30k_images/{temp_df.image_name[i]}\")\n",
        "        plt.imshow(image.permute(1, 2, 0))\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.comment[i], 30)))\n",
        "\n",
        "#num = 5\n",
        "#display_images(raw_df.sample(num))"
      ],
      "metadata": {
        "id": "MfB4KsAgZTrB"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocabulary for Captions\n",
        "def build_vocab(captions,tokenizer,max_len):\n",
        "  counter = Counter()\n",
        "  current_max = max_len\n",
        "  for caption in captions:\n",
        "    sentence = caption.strip()\n",
        "    tokens = tokenizer(sentence)\n",
        "    if len(tokens) > current_max:\n",
        "      current_max = len(tokens)\n",
        "    counter.update(tokens)\n",
        "  sorted_by_freq = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
        "  dictionary = OrderedDict(sorted_by_freq)\n",
        "  dictionary = dict(dictionary)\n",
        "  dictionary['<PAD>'] = len(dictionary)\n",
        "  # dictionary['SOS'] = len(dictionary) + 1\n",
        "  # dictionary['EOS'] = len(dictionary) + 2\n",
        "  # dictionary['<UNK>'] = len(dictionary) + 3\n",
        "  dictionary = OrderedDict(dictionary)\n",
        "  return vocab(dictionary), current_max"
      ],
      "metadata": {
        "id": "E2vsl3IoFom4"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_dir, label_csv, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.target_len = -1\n",
        "\n",
        "        self.data_df = pd.read_csv(label_csv, sep='|',header=None)\n",
        "        self.data_df = self.data_df.dropna()\n",
        "        #self.data_df.drop([' comment_number'],axis=1, inplace=True)\n",
        "        #self.data_df.columns = self.data_df.columns.str.replace(' ', '')\n",
        "        self.captions = self.data_df[1] # 0 is image_name, 1 is comment\n",
        "\n",
        "        self.transform = transform\n",
        "        self.image_paths = self.data_df[0]#os.listdir(data_dir)  # Assumes images are directly under data_dir\n",
        "\n",
        "        self.tokenizer = get_tokenizer(\"basic_english\")\n",
        "        self.vocab, self.target_len = build_vocab(self.captions.tolist(),self.tokenizer,self.target_len)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.data_dir, self.image_paths[idx])\n",
        "        #image = cv2.imread(image_path)\n",
        "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        desired_size = 128 # 128x128\n",
        "        image = cv2.imread(image_path)\n",
        "        old_size = image.shape[:2]\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = (image - np.mean(image))/np.std(image) # normalize\n",
        "        ratio = float(desired_size)/max(old_size)\n",
        "        new_size = tuple([int(x*ratio) for x in old_size])\n",
        "        image = cv2.resize(image, (new_size[1], new_size[0]))\n",
        "        delta_w = desired_size - new_size[1]\n",
        "        delta_h = desired_size - new_size[0]\n",
        "        top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
        "        left, right = delta_w//2, delta_w-(delta_w//2)\n",
        "        new_im = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "        transform = transforms.ToTensor()\n",
        "        tensor = transform(new_im)\n",
        "        #image = tensor.ToPILImage()\n",
        "        image = tensor\n",
        "        #image = Image.open(image_path).convert(\"RGB\")\n",
        "        caption = self.captions[idx]\n",
        "        \n",
        "        #if self.transform is not None:\n",
        "        #    image = self.transform(image)\n",
        "\n",
        "        # Vectorize the given caption using our dataset's vocabulary\n",
        "        caption = caption.strip()\n",
        "        caption = self.tokenizer(caption)\n",
        "        caption_vector = [self.vocab[word] for word in caption]\n",
        "\n",
        "        # Add padding to the vector if it needs it\n",
        "        if len(caption_vector) < self.target_len:\n",
        "          for i in range(self.target_len - len(caption_vector)):\n",
        "            caption_vector.append(self.vocab['<PAD>'])\n",
        "\n",
        "        # Return the processed image and any associated labels\n",
        "        return image, torch.tensor(caption_vector)\n"
      ],
      "metadata": {
        "id": "pKYERZ5AUtFM"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    #transforms.Resize((128, 128)),\n",
        "    transforms.ToPILImage()\n",
        "])\n",
        "\n",
        "data_dir = '/content/flickr30k_images/'\n",
        "label_csv = '/content/drive/MyDrive/ImageCaptioningDataset/results.csv'\n",
        "dataset = CustomDataset(data_dir, label_csv, transform=transform)\n",
        "len(dataset)"
      ],
      "metadata": {
        "id": "vxtZgwO6e21r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddaf2e0b-82f0-4cde-c1b6-6cfa1010ad95"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "158915"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test,train = random_split(dataset, [int(len(dataset)*0.3)+1, int(len(dataset)*0.7)])\n",
        "print(len(test),len(train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLFtN18AqHI2",
        "outputId": "b358e057-edfc-4707-c66b-da6040d5b32a"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47675 111240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = raw_df[0]\n",
        "image_paths[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jZGiNPk2eyir",
        "outputId": "586e46c6-6984-4b72-d47e-5f003664dba3"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1000092795.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 37\n",
        "#dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EfbFZUHSVJ16"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# i = 0\n",
        "# for img, file_name in dataloader:\n",
        "#   clear_output()\n",
        "#   #print(label[0])\n",
        "#   #print(raw_df['image_name'])\n",
        "#   plt.imshow(img[0].permute(1,2,0))\n",
        "#   print(list(raw_df.loc[raw_df['image_name'] == file_name[0]].comment)[0])\n",
        "#   i+=5\n",
        "#   plt.show()\n",
        "#   time.sleep(1)"
      ],
      "metadata": {
        "id": "8Om2xhbrV8Kf"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neual Network (CNN)"
      ],
      "metadata": {
        "id": "NwlKzpy10_cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tXi1uiPd0_XR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(dataset.vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "bkHmM37T9822",
        "outputId": "305ec490-13b4-401f-aaae-97b0d28b9326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20217"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCaptioningModel(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "        \n",
        "        # Load the pretrained ResNet-101 model\n",
        "        self.resnet = models.wide_resnet50_2(weights=None)\n",
        "        self.resnet.fc = nn.Linear(2048, embed_size)\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.resnet(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        \n",
        "        embeddings = self.embedding(captions)\n",
        "        \n",
        "        combined = torch.cat((features, embeddings), dim=1)\n",
        "        \n",
        "        lstm_out, _ = self.lstm(combined)\n",
        "        \n",
        "        outputs = self.fc(lstm_out)\n",
        "        \n",
        "        return outputs"
      ],
      "metadata": {
        "id": "6vCRizEKHyg0"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 256  # Size of the word embedding\n",
        "hidden_size = 512  # Size of the LSTM hidden state\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device('mps') if torch.backends.mps.is_available() else device\n",
        "\n",
        "model = ImageCaptioningModel(embed_size, hidden_size, vocab_size).to(device)"
      ],
      "metadata": {
        "id": "v04TdT8KIShA"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(x, y, a):\n",
        "    lam = np.random.beta(a,a)\n",
        "    rand = torch.randperm(batch_size)\n",
        "    x2 = x[rand,:]\n",
        "    y2 = y[rand,:]\n",
        "    X = lam * x + (1 - lam) * x2\n",
        "    Y = lam * y + (1 - lam) * y2\n",
        "    print(Y)\n",
        "    return X, Y\n",
        "\n",
        "def cutout(image, k):\n",
        "    x = np.random.randint(0, 128-k)\n",
        "    y = np.random.randint(0, 128-k)\n",
        "    image[:, x:x+k, y:y+k] = 0\n",
        "    return image\n",
        "\n",
        "def standard_augmentation(image,k):\n",
        "    k1 = np.random.randint(-k, k)\n",
        "    k2 = np.random.randint(-k, k)\n",
        "    image = np.roll(image, k1, axis=1)\n",
        "    image = np.roll(image, k2, axis=2)\n",
        "    if (k1 > 0):    \n",
        "        image[:, :k1, :] = 0\n",
        "    else:\n",
        "        image[:, k1:, :] = 0\n",
        "    if (k2 > 0):\n",
        "        image[:, :, :k2] = 0\n",
        "    else:\n",
        "        image[:, :, k2:] = 0\n",
        "    if (np.random.rand() < 0.5):\n",
        "        image = np.flip(image, axis=2)\n",
        "    return image"
      ],
      "metadata": {
        "id": "4Y9TNrlpu_NE"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "def training():\n",
        "  for epoch in range(num_epochs):\n",
        "      i = 0\n",
        "      copy_train_loader = deepcopy(train_loader)\n",
        "      for images, captions in copy_train_loader:\n",
        "          # Move images and captions to device (e.g., GPU)\n",
        "          #if (np.random.rand() < 0.5):\n",
        "                #image = X_epoch[j].cpu()\n",
        "                #X_epoch[j] = cutout(image, 16)\n",
        "              #image = X_epoch[j].cpu()\n",
        "              #X_epoch[j] = torch.from_numpy(standard_augmentation(image, 4).copy()).to(device)\n",
        "            #X_epoch, Y_epoch = mixup(X_epoch, Y_epoch, alpha)\n",
        "          images = images.to(device, dtype=torch.float)\n",
        "          for j in range(len(images)):\n",
        "            if (np.random.rand() < 0.5):\n",
        "              images[j] = cutout(images[j], 64)\n",
        "            images[j] = torch.from_numpy(standard_augmentation(images[j].cpu(),16).copy())\n",
        "          #images, captions = mixup(images, captions, 0.4)\n",
        "\n",
        "          captions = captions.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images, captions[:, :-1])\n",
        "\n",
        "          # Reshape captions for loss calculation\n",
        "          targets = captions[:, :].reshape(-1)\n",
        "          \n",
        "          # Compute loss\n",
        "          loss = criterion(outputs.reshape(-1, vocab_size), targets)\n",
        "\n",
        "          # Backward pass and optimization\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          # Print the loss\n",
        "          if i % 100 == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i, len(train_loader), loss.item()))\n",
        "          i+=1\n",
        "\n",
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/ImageCaptioningDataset/model_path.pt'), strict=False)\n",
        "training()"
      ],
      "metadata": {
        "id": "v3eCeIOEIjTI",
        "outputId": "0056df4e-3498-432a-a3bf-33b37f169e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Step [0/3007], Loss: 10.0643\n",
            "Epoch [1/10], Step [100/3007], Loss: 0.8347\n",
            "Epoch [1/10], Step [200/3007], Loss: 0.7096\n",
            "Epoch [1/10], Step [300/3007], Loss: 0.7331\n",
            "Epoch [1/10], Step [400/3007], Loss: 0.7334\n",
            "Epoch [1/10], Step [500/3007], Loss: 0.6525\n",
            "Epoch [1/10], Step [600/3007], Loss: 0.7267\n",
            "Epoch [1/10], Step [700/3007], Loss: 0.7227\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normal_image = dataset[0][0]\n",
        "image = dataset[0][0].unsqueeze(0)\n",
        "caption = dataset[0][1].unsqueeze(0)\n",
        "image = image.to(device)\n",
        "caption = caption.to(device)"
      ],
      "metadata": {
        "id": "W_lfCl54RXuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  outputs = model(image,caption)\n",
        "  _,preds = torch.max(outputs,2)"
      ],
      "metadata": {
        "id": "fJcMPk-URYHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translated_preds = []\n",
        "for _,num in enumerate(preds[0]):\n",
        "  idx = num.item()\n",
        "  if idx != 20216:\n",
        "    translated_preds.append(dataset.vocab.lookup_token(idx))\n",
        "\n",
        "pred_caption = ' '.join(translated_preds)\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "unt9VMcIS9mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(normal_image.permute(1, 2, 0))\n",
        "print(pred_caption)"
      ],
      "metadata": {
        "id": "H3gNp2TbXp7C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}